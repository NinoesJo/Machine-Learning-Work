{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Logistic\n",
    "\n",
    "In this part, you will complete a Do-It-Yourself (DIY) implementation of binary logistic regression in an object-oriented pattern that corresponds with the Scikit-Learn API.\n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Write object-oriented code for a Python class, matching standard API patterns.\n",
    "2. Apply numerical Python (NumPy) to efficiently implement binary logistic regression, including code to fit the model to data using the gradient descent algorithm. \n",
    "3. Evaluate your implementation compared to the Scikit-Learn standard on synthetic data. \n",
    "4. Perform an ablation study on the impact of the learning rate hyperparameter for fitting a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Before implementing logistic regression, let's understand the mathematical foundations.\n",
    "\n",
    "**Sigmoid Function**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ transforms any real value $z$ into a probability between 0 and 1.\n",
    "\n",
    "**Logistic Regression Model**: For features $\\mathbf{x}$ and weights $\\mathbf{w}$:\n",
    "$$P(y = 1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x})$$\n",
    "\n",
    "**Decision Boundary**: The line/plane where $\\mathbf{w}^T \\mathbf{x} = 0$ (i.e., $P(y=1) = 0.5$).\n",
    "\n",
    "**Cross-Entropy Loss**: For a given data point true label $y \\in \\{0,1\\}$ and predicted probability $p$:\n",
    "$$\\text{Loss} = -[y \\log(p) + (1-y) \\log(1-p)]$$\n",
    "\n",
    "\n",
    "### Worked Example\n",
    "\n",
    "Suppose you have four training data points $x^{(1)}, x^{(2)}, x^{(3)}$, and $x^{(4)}$. Each data point $x^{(i)}$ has two input features $x^{(i)}_1$ and $x^{(i)}_2$ and a binary (0 or 1) predictive target $y_i$.\n",
    "\n",
    "**Data Points**\n",
    "- $x^{(1)} = [2, 1], \\quad y_1 = 1$\n",
    "- $x^{(2)} = [1, 3], \\quad y_2 = 0$ \n",
    "- $x^{(3)} = [3, 0], \\quad y_3 = 1$\n",
    "- $x^{(4)} = [0, 2], \\quad y_4 = 1$ (this will be misclassified)\n",
    "\n",
    "**Model Parameters:**\n",
    "- Weights: $w_1 = 1, w_2 = -1$\n",
    "- Bias: $b = 0$ *(Note: Bias terms are not required for this assignment)*\n",
    "\n",
    "This implies that the logits will be $z = w_1x_1 + w_2x_2 + b = x_1 - x_2$\n",
    "\n",
    "#### Example Step-by-Step Calculations\n",
    "\n",
    "**Point 1: $x^{(1)} = [2, 1], y_1 = 1$**\n",
    "- $z_1 = 1(2) + (-1)(1) + 0 = 2 - 1 = 1$\n",
    "- $p_1 = \\sigma(1) = \\frac{1}{1 + e^{-1}} = \\frac{1}{1 + 0.368} = 0.731$\n",
    "- Predicted class: $\\hat{y}_1 = 1$ (since $p_1 > 0.5$)\n",
    "- Loss: $L_1 = -(1 \\cdot \\log(0.731) + 0 \\cdot \\log(0.269)) = -\\log(0.731) = 0.313$\n",
    "\n",
    "**Point 4: $x^{(4)} = [0, 2], y_4 = 1$ (Misclassified)**\n",
    "- $z_4 = 1(0) + (-1)(2) + 0 = 0 - 2 = -2$\n",
    "- $p_4 = \\sigma(-2) = \\frac{1}{1 + e^{2}} = \\frac{1}{1 + 7.389} = 0.119$\n",
    "- Predicted class: $\\hat{y}_4 = 0$ (since $p_4 < 0.5$) ❌ **MISCLASSIFIED**\n",
    "- Loss: $L_4 = -(1 \\cdot \\log(0.119) + 0 \\cdot \\log(0.881)) = -\\log(0.119) = 2.127$\n",
    "\n",
    "#### Summary Results\n",
    "\n",
    "| Point | Features | True $y$ | $z$ | $P(y=1)$ | Predicted $\\hat{y}$ | Loss | Correct? |\n",
    "|-------|----------|----------|-----|----------|-------------------|------|----------|\n",
    "| 1     | [2, 1]   | 1        | 1   | 0.731    | 1                 | 0.313| ✓        |\n",
    "| 2     | [1, 3]   | 0        | -2  | 0.119    | 0                 | 0.127| ✓        |\n",
    "| 3     | [3, 0]   | 1        | 3   | 0.953    | 1                 | 0.048| ✓        |\n",
    "| 4     | [0, 2]   | 1        | -2  | 0.119    | 0                 | 2.127| ❌       |\n",
    "\n",
    "#### Performance Metrics\n",
    "\n",
    "**Total Cross-Entropy Loss:** $L_{total} = 0.313 + 0.127 + 0.048 + 2.127 = 2.615$\n",
    "\n",
    "**Mean Cross-Entropy Loss:** $L_{mean} = \\frac{2.615}{4} = 0.654$\n",
    "\n",
    "**Accuracy:** $\\frac{3 \\text{ correct}}{4 \\text{ total}} = 75\\%$\n",
    "\n",
    "#### Observations\n",
    "\n",
    "1. **Decision Boundary:** The decision boundary is the line $x_1 - x_2 = 0$, or $x_1 = x_2$.\n",
    "\n",
    "2. **Misclassification Impact:** Point 4 has the highest cross-entropy loss (2.127) because it's confidently misclassified—the model predicts probability 0.119 for a true positive case.\n",
    "\n",
    "3. **Loss vs. Accuracy:** While accuracy is 75%, the cross-entropy loss captures the confidence of predictions. Point 3, though correctly classified, has low loss (0.048) because the model is very confident in its correct prediction.\n",
    "\n",
    "4. **Geometric Interpretation:** Points above the line $x_1 = x_2$ are classified as positive (class 1), while points below are classified as negative (class 0). Point 4 at [0, 2] falls below this line but should be classified as positive, hence the misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "First, we will use Scikit-Learn to develop a baseline logistic regression model to which we can compare our DIY implementation. Run the following code to generate synthetic data for use in this part of the assignment. Observe that the predictive target is coded as 0 or 1, that the `sigmoid` function is defined for you, and that the code also splits the synthetic data into train and test sets for you.\n",
    "\n",
    "Use Scikit-Learn to fit a [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#logisticregression) on the train set with the parameter setting `penalty = 'None'` (this will train a basic model without applying any regularization). Evaluate and report the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of the model on both the train set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run but do not modify this code\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "np.random.seed(2025)\n",
    "n = 1000\n",
    "features = 20\n",
    "\n",
    "X = np.random.normal(size = (n, features))\n",
    "weights = np.random.normal(size = features)\n",
    "probs = sigmoid(X @ weights + np.random.normal(scale=0.01)) \n",
    "y = np.random.binomial(n=1, p=probs)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the train set is 0.85\n",
      "The accuracy of the model on the test set is 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Write code for task 1 here\n",
    "\n",
    "logistic_regression_model = LogisticRegression(penalty = None, random_state = 2025)\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "y_train_prediction = logistic_regression_model.predict(X_train)\n",
    "y_test_prediction = logistic_regression_model.predict(X_test)\n",
    "train_accuracy = accuracy_score(y_train, y_train_prediction)\n",
    "test_accuracy = accuracy_score(y_test, y_test_prediction)\n",
    "print(\"The accuracy of the model on the train set is\", train_accuracy)\n",
    "print(\"The accuracy of the model on the test set is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Complete the following class to implement binary logistic regression. Some important notes about the implementation:\n",
    "\n",
    "1. Remember that the Scikit-Learn API treats an input `X` array, whether to `fit` or `predict`, as a design matrix with a row for every data point and a column for every feature. \n",
    "\n",
    "2. For `fit`, every row in `X` corresponds to a given output in `y`, and  you don't need to return anything, just optimize the internal model weights (which should be stored as instance variables). For `predict_proba` and `predict`, you should return a NumPy array with one element (corresponding to a probability or a 0/1 value) for every row in the input `X`.\n",
    "\n",
    "3. Remember that logistic regression models the probability of outputting `1` as a sigmoid of a linear function of features. This has several implications. \n",
    "    - One is that the number of weights in your model should equal the number of features, which equals the number of columns in the `X` matrix passed to the `fit` method. We recommend that you initialize these weights as random normally distributed values, for example by using NumPy's [random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html). \n",
    "    - Another implication is that `predict_proba` should return the sigmoid activation of the [dot product](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) (multiply element-wise then add together) of your model's weights and the given input. You do **not** need to include a bias term for this implementation.\n",
    "\n",
    "4. For the `predict` function, you should use a simple thresholding of 0.5. That is, you should calculate the probabilities using the `predict_proba` method and return `1` if the probability is greater than 0.5 and `0` otherwise. You can assume that `y` will consist exclusively of `0`s or `1`s for the purpose of this implementation.\n",
    "\n",
    "5. The `fit` method should implement gradient descent on the cross entropy loss. The process is described below followed by hints about vectorized NumPy operations. \n",
    "    - For a given feature/weight dimension $j$ and a particular data point $x$ with label $y$, the partial derivative with respect to weight $w_j$ is $(a - y)x_j$ where $a$ is the activation (the predicted probability) associated with example $x$. \n",
    "    - For each feature, this quantity should be averaged over all training data. The vector of all such values forms the gradient $\\vec{\\nabla}$. \n",
    "    - The gradient descent learning update should then be $\\vec{w'} = \\vec{w} - \\eta \\vec{\\nabla}$ where $\\eta$ is the learning rate `lr` passed to the constructor, $\\vec{w}$ are the previous weights and $\\vec{w'}$ are the weights for the next iteration. \n",
    "    - The algorithm should proceed for `max_iters` iterations unless the magnitude of the gradient becomes less than the `tol` hyperparameter. This is implemented in the code for you.\n",
    "\n",
    "6. Vectorization hint: Instead of using nested loops, use NumPy matrix operations. The key insight is that you can compute gradients for all features simultaneously:\n",
    "   - `X @ weights` gives you all linear combinations: shape `(n_samples,)`\n",
    "   - `sigmoid(X @ weights)` gives you all predictions: shape `(n_samples,)`\n",
    "   - `X.T @ (predictions - y)` gives you gradients for all features: shape `(n_features,)`\n",
    "   - Don't forget to divide by the number of samples to get the average gradient\n",
    "\n",
    "7. You will note the `fit` method takes an optional `verbose` parameter. While it is not required, we highly recommend that you include code in the `fit` method that, when `verbose` is `True`, provides additional logging or printing of information about the training process to help debug. The code already shown prints the magnitude of the gradient every 10 iterations.\n",
    "\n",
    "8. The `pass` statements are syntactic placeholders that should be removed when you implement a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, lr=0.1, max_iters=1000, random_state=2025, tol=1e-6):\n",
    "        self.lr = lr\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state \n",
    "        self.tol = tol  # Tolerance for checking convergence\n",
    "        self.weights = None # Number of weights determined in fit\n",
    "\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Clip z to prevent numerical overflow/underflow\n",
    "        z_clipped = np.clip(z, -500, 500)\n",
    "        return 1.0/(1.0 + np.exp(-z_clipped))\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probability of 1 for each row in X\"\"\"\n",
    "        # todo: complete predict_proba method\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Make sure to call fit(X, y, verbose) first.\")\n",
    "        linear_combinations = X @ self.weights\n",
    "        predictions = self.sigmoid(linear_combinations)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class label 1 or 0 for each row in X\"\"\"\n",
    "        # Hint: Use predict_proba and apply 0.5 threshold\n",
    "        # Hint: Result should have shape (n_samples,)\n",
    "        # todo: complete predict method\n",
    "        probabilities = self.predict_proba(X)\n",
    "        class_labels = (probabilities > 0.5).astype(int)\n",
    "        return class_labels\n",
    "\n",
    "\n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\" Fit the training data with gradient descent.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values, assumed to be 0 or 1.\n",
    "        verbose : bool, optional (default=False)\n",
    "          If True, print training progress information.\n",
    "        \"\"\"\n",
    "        # Set random seed for reproducible weight initialization\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Initialize weights\n",
    "        # todo: initialize self.weights using np.random.normal\n",
    "        self.weights = np.random.normal(size = X.shape[1])\n",
    "        \n",
    "        # Gradient descent loop\n",
    "        for i in range(self.max_iters):\n",
    "            # todo: calculate gradient\n",
    "            linear_predictions = X @ self.weights\n",
    "            predicted_probabilities = self.sigmoid(linear_predictions)\n",
    "            prediction_errors = predicted_probabilities - y\n",
    "            gradient = (X.T @ prediction_errors) / X.shape[0]\n",
    "            \n",
    "            # Check for convergence -- update gradient variable name if different\n",
    "            gradient_magnitude = np.linalg.norm(gradient)\n",
    "            if gradient_magnitude < self.tol:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at iteration {i}, gradient magnitude: {gradient_magnitude:.2e}\")\n",
    "                break\n",
    "                \n",
    "            # todo: update self.weights using gradient descent rule\n",
    "            self.weights -= self.lr * gradient\n",
    "            \n",
    "            if verbose and i % 10 == 0:\n",
    "                print(f\"Iteration {i}, gradient magnitude: {gradient_magnitude:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Use your DIY `BinaryLogisticRegression` class from task 2 to fit a logistic regression model on the train set as you did for the Scikit-Learn implementation in task 1. Use the default parameters.\n",
    "\n",
    "Evaluate and report the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of your DIY model on both the train set and the test set. You should achieve similar performance (within a few percent, like 2%) compared to the Scikit-Learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the DIY model on the train set is 0.8457142857142858\n",
      "The accuracy of the DIY model on the test set is 0.79\n",
      "The accuracy of the Scikit-Learn model on the train set is 0.85\n",
      "The accuracy of the Scikit-Learn model on the test set is 0.8\n"
     ]
    }
   ],
   "source": [
    "# Write code for task 3 here\n",
    "DIY_model = BinaryLogisticRegression()\n",
    "DIY_model.fit(X_train, y_train)\n",
    "y_train_prediction_DIY = DIY_model.predict(X_train)\n",
    "y_test_prediction_DIY = DIY_model.predict(X_test)\n",
    "DIY_train_accuracy = accuracy_score(y_train, y_train_prediction_DIY)\n",
    "DIY_test_accuracy = accuracy_score(y_test, y_test_prediction_DIY)\n",
    "print(\"The accuracy of the DIY model on the train set is\", DIY_train_accuracy)\n",
    "print(\"The accuracy of the DIY model on the test set is\", DIY_test_accuracy)\n",
    "print(\"The accuracy of the Scikit-Learn model on the train set is\", train_accuracy)\n",
    "print(\"The accuracy of the Scikit-Learn model on the test set is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (Bonus)\n",
    "\n",
    "Perform an *ablation* study on the learning rate hyperparameter `lr`. Specifically, try fitting your DIY `BinaryLogisticRegression` implementation seven different times with different settings of the learning rate hyperparameter `lr`. Try each of the values `[100, 10, 1, 0.1, 0.01, 0.001, 0.0001]`.\n",
    "\n",
    "For each run:\n",
    "  - Evaluate the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of the model predictions on the train set only (note that hyperparameters should never be selected using the test data).\n",
    "  - Record how many iterations were necessary to fit the model (either by reaching the convergence criterion or just the `max_iters` limit).\n",
    "\n",
    "Report all of your results. Based on your findings, briefly explain the importance of selecting a good learning rate, considering both model performance and computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for task 4 here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain for Task 4 here*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
