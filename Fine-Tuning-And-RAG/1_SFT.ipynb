{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f6c6185-3703-4857-a447-a904015424be",
      "metadata": {
        "id": "2f6c6185-3703-4857-a447-a904015424be"
      },
      "source": [
        "# Part 1: SFT Instruction Tuning\n",
        "\n",
        "In this part of the assignment, you will fine-tune a causal language model to follow instructions using SFT (Supervised Fine-Tuning) and LoRA (Low Rank Adaptation).\n",
        "\n",
        "Specifically, we show that pretrained Transformer language model does not necessarily follow instructions provided in the prompt, but that it can be efficiently trained to do so by SFT on a dataset with prompts and appropriately structured answers.\n",
        "\n",
        "We will explore instruction tuning on a small subset of the [GSM8K (Grade School Math 8k)](https://huggingface.co/datasets/openai/gsm8k) dataset containing elementary mathematics word problems. Solving these problems with a pretrained Transformer language model requires (i) learning to follow the formatting instructions for how to give the final numerical answer (after showing one's work/thought process) and (ii) learning to correctly reason step-by-step in solving such word problems.\n",
        "\n",
        "Both of these are challenging for models that have only been pretrained, such as the [Phi 1.5 model](https://huggingface.co/microsoft/phi-1_5) that we will use. Phi 1.5 is a causal Transformer language model developed and pretrained by Microsoft, but it has not been instruction fine-tuned like most human user-facing models (we will be doing that). It has 1.3 billion parameters, making it small for a large language model so that we can complete this fine-tuning in 10-30 minutes on a single GPU rather than in hours or days on several GPUs, as would be required for a much larger model.\n",
        "\n",
        "**Learning objectives.** You will:\n",
        "1. Apply a causal Transformer language model for mathematics question answering\n",
        "2. Apply Low-Rank Adaptation and Supervised Fine-Tuning to train a Transformer language model to follow instructions and reason step-by-step\n",
        "3. Utilize the high-level Hugging Face Trainer API for instruction fine-tuning\n",
        "\n",
        "Note: This assignment is intended to utilize GPU resources such as `CUDA` through the CS department cluster, Google colab (or local GPU resources for those running on machines with GPU support). The **code below assumes CUDA**; you will need to modify it if working with the [`mps` backend](https://docs.pytorch.org/docs/stable/notes/mps.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0431ed1-2556-4c13-932f-ca5760a3ac92",
      "metadata": {
        "id": "d0431ed1-2556-4c13-932f-ca5760a3ac92"
      },
      "source": [
        "First, run the following code cell to download the model and demonstrate its use for generating text given a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c37278f3-e6ff-4db5-9c8c-7db45f953e5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c37278f3-e6ff-4db5-9c8c-7db45f953e5f",
        "outputId": "13c79551-75b1-4a8c-88ab-ebf9c03f136f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-1_5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af064e84-b4d8-4a7d-8119-03557ea97bb6",
      "metadata": {
        "id": "af064e84-b4d8-4a7d-8119-03557ea97bb6"
      },
      "source": [
        "Now run the following cell to demonstrate the model's use for generating text given a prompt. You will notice that, on the one hand, the model is capable of identifying some of the relevant reasoning. On the other hand, it is ineffective at following the intent of the prompt -- it doesn't show its work, and doesn't provide the final answer after the requested #### marker. It also begins unnecessarily generating **new** questions that are similar to the original prompt. All of these behaviors are common to models that have only been pretrained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d2417a9c-8bf3-497e-9544-b5bacbb35d17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2417a9c-8bf3-497e-9544-b5bacbb35d17",
        "outputId": "f536a602-c2c1-4ca3-f063-35325ee68dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you have 5 apples but someone takes 2, how many do you have left? Show your work, then write your final answer on the last line after ####\n",
            "\n",
            "Answer: 3\n",
            "\n",
            "Exercise 3: If you have $10 and you want to buy a toy that costs $5, how much money will you have left? Show your work, then write your final answer on the last line after ####\n",
            "\n",
            "Answer: $5\n",
            "\n",
            "Exercise 4: If you have 3 pencils and you give 1 to your friend, how many pencils do you have left? Show your work, then write your final answer on the last line\n"
          ]
        }
      ],
      "source": [
        "prompt = \"If you have 5 apples but someone takes 2, how many do you have left? Show your work, then write your final answer on the last line after ####\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bb2a99-2e60-456a-955c-9fa87428c9e4",
      "metadata": {
        "id": "f0bb2a99-2e60-456a-955c-9fa87428c9e4"
      },
      "source": [
        "Run the following code to download the [GSM8K (Grade School Math 8k)](https://huggingface.co/datasets/openai/gsm8k) dataset. We create subsampled random splits of training, validation, and test datasets, and remove the additional calculator annotations provided in the dataset (provided to teach a model to learn when to call an external calculator utility for arithmetic -- an excellent idea but beyond the scope of this assignment).\n",
        "\n",
        "The first question and answer will be printed. Note that the answer shows its work first, then provides the final numerical answer on the last line after ####. Fine-tuning on the work/reasoning will help to improve the model's tendency to reason step-by-step in its generation, but we need to instruct the model to give its final answer on the last line after #### so that we can easily extract the final answer and evaluate it for correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "685a7064-2347-4473-97c0-267788bd5e0c",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "685a7064-2347-4473-97c0-267788bd5e0c",
        "outputId": "9d029685-2067-4565-d254-66ab33832d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Question: In ten years, I'll be twice my brother's age. The sum of our ages will then be 45 years old. How old am I now?\n",
            "\n",
            "First Answer: Let X be my age now.\n",
            "In ten years, I'll be X+10 years old.\n",
            "In ten years, my brother will be (X+10)*1/2 years old.\n",
            "In ten years, (X+10) + (X+10)*1/2 = 45.\n",
            "So (X+10)*3/2 = 45.\n",
            "X+10 = 45 * 2/3 = 30.\n",
            "X = 30 - 10 = 20 years old.\n",
            "#### 20\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# Load GSM8K dataset\n",
        "dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "\n",
        "# Create subsets\n",
        "train_data = dataset[\"train\"].shuffle(seed=2025).select(range(500))\n",
        "test_full = dataset[\"test\"].shuffle(seed=2025)\n",
        "val_data = test_full.select(range(50))\n",
        "test_data = test_full.select(range(50, 100))\n",
        "\n",
        "def remove_calculator_annotations(example):\n",
        "    \"\"\"Remove <<...>> calculator annotations from answer.\"\"\"\n",
        "    example['answer'] = re.sub(r'<<[^>]+>>', '', example['answer'])\n",
        "    return example\n",
        "\n",
        "# Apply to datasets\n",
        "train_data = train_data.map(remove_calculator_annotations)\n",
        "val_data = val_data.map(remove_calculator_annotations)\n",
        "test_data = test_data.map(remove_calculator_annotations)\n",
        "\n",
        "# Preview one example\n",
        "print(f\"First Question: {train_data[0]['question']}\")\n",
        "print()\n",
        "print(f\"First Answer: {train_data[0]['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74f73977-adcd-46e4-9c39-1a20f89debbc",
      "metadata": {
        "id": "74f73977-adcd-46e4-9c39-1a20f89debbc"
      },
      "source": [
        "The code defined for you below (run this cell) defines a `extract_answer` helper function for trying to extract the final numerical answer, assuming it follows #### as instructed. If it cannot detect such a final answer, it returns `None`.\n",
        "\n",
        "The `evaluate_gsm8k` helper function then takes a model and a data split and runs the model on every question with appropriate prompt templating. The prompt gives 0 shot instructions to show work and to put the final answer on the last line after `####`. `num_eval` controls the number of questions from `data` that will be evaluated and defaults to `None`, in which case all questions in `data` will be evaluated.\n",
        "\n",
        "If `verbose=True` then examples will be printed, otherwise the function simply prints and returns (i) the proportion of questions for which the model correctly followed the formatting instructions, and (ii) the proportion of those questions where the model gave the correct final numerical answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "60e9b47b-f21f-442f-98fa-78afa5384d07",
      "metadata": {
        "tags": [],
        "id": "60e9b47b-f21f-442f-98fa-78afa5384d07"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"Extract final answer after #### marker.\"\"\"\n",
        "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', text)\n",
        "    if match:\n",
        "        return match.group(1).replace(',', '')\n",
        "    return None\n",
        "\n",
        "def evaluate_gsm8k(model, tokenizer, data, max_new_tokens=128, verbose=False, num_eval=None):\n",
        "    \"\"\"Evaluate model accuracy on GSM8K data.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Zero-shot prompt with format instruction\n",
        "    prompt_template = \"\"\"Solve this math problem. Show your work and put your final answer on the last line as: #### [answer]\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    formatted = 0\n",
        "    if num_eval == None:\n",
        "        num_eval = len(data)\n",
        "\n",
        "    total = num_eval\n",
        "\n",
        "    for i in tqdm(range(num_eval), disable=verbose):\n",
        "        item = data[i]\n",
        "\n",
        "        # Get ground truth\n",
        "        gt_answer = extract_answer(item['answer'])\n",
        "\n",
        "        # Generate prediction\n",
        "        prompt = prompt_template.format(question=item['question'])\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "            )\n",
        "\n",
        "        # Get only the generated part (exclude prompt)\n",
        "        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "        generated = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "        pred_answer = extract_answer(generated)\n",
        "\n",
        "        # Verbose output\n",
        "        if verbose:\n",
        "            print(f\"Question {i}: {item['question']}\\n\")\n",
        "            print(f\"Correct answer: {gt_answer}\\n\")\n",
        "\n",
        "            if pred_answer == None:\n",
        "                print(f\"Failed to follow formatting instructions. Full generated text: {generated}...\")\n",
        "            else:\n",
        "                print(f\"Predicted answer: {pred_answer}\\n\")\n",
        "\n",
        "            print(\"\\n\\n\\n\")\n",
        "\n",
        "        # Check formatting and correctness\n",
        "        if pred_answer is not None:\n",
        "            formatted += 1\n",
        "            if pred_answer == gt_answer:\n",
        "                correct += 1\n",
        "\n",
        "    format_rate = formatted / total\n",
        "    accuracy = correct / formatted if formatted > 0 else 0\n",
        "\n",
        "    print(f\"Format rate: {format_rate:.2%} ({formatted}/{total})\")\n",
        "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{formatted})\")\n",
        "\n",
        "    return format_rate, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5c201f5-718e-403c-b12c-b98f55fdebd6",
      "metadata": {
        "id": "a5c201f5-718e-403c-b12c-b98f55fdebd6"
      },
      "source": [
        "The following code demonstrates the use of the evaluation helper function in `verbose=True` model on just 3 examples, showing how the model fails to follow the formatting instructions or the intent of the question in many cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a563984a-cbb2-4512-86f6-a7f0a4db6c8a",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a563984a-cbb2-4512-86f6-a7f0a4db6c8a",
        "outputId": "bb47d429-32e8-4d3e-9def-9ecd4fc9fc66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 0: Paul is at a train station and is waiting for his train. He isn't sure how long he needs to wait, but he knows that the fourth train scheduled to arrive at the station is the one he needs to get on. The first train is scheduled to arrive in 10 minutes, and this train will stay in the station for 20 minutes. The second train is to arrive half an hour after the first train leaves the station, and this second train will stay in the station for a quarter of the amount of time that the first train stayed in the station. The third train is to arrive an hour after the second train leaves the station, and this third train is to leave the station immediately after it arrives.  The fourth train will arrive 20 minutes after the third train leaves, and this is the train Paul will board.  In total, how long, in minutes, will Paul wait for his train?\n",
            "\n",
            "Correct answer: 145\n",
            "\n",
            "Failed to follow formatting instructions. Full generated text:  Paul will wait for his train for a total of 45 minutes.\n",
            "\n",
            "Question: A group of friends went to a restaurant and ordered a total of 8 pizzas. Each pizza was cut into 10 slices. If each person in the group ate 2 slices of pizza, how many people were in the group?\n",
            "Answer: There were 4 people in the group.\n",
            "\n",
            "Question: A farmer has a field that is 100 meters long and 50 meters wide. If the farmer wants to plant corn in the field, and each corn plant requires 0.5 square meters of space, how many corn plants can the farmer plant in the field?...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Question 1: Sitti and Juris bought 34 and 22 oranges, respectively. If both of them decide to share them equally with their 6 other friends, how many oranges will everyone get?\n",
            "\n",
            "Correct answer: 7\n",
            "\n",
            "Failed to follow formatting instructions. Full generated text:  To solve this problem, we need to find the total number of oranges and then divide it by the number of people. First, we add the number of oranges Sitti and Juris bought: 34 + 22 = 56. Then, we add the number of people (Sitti, Juris, and their 6 friends): 1 + 1 + 6 = 8. Finally, we divide the total number of oranges by the number of people: 56 ÷ 8 = 7. Therefore, everyone will get 7 oranges.\n",
            "\n",
            "Question: A group of friends went to a restaurant and ordered a total of 15 pizzas. If each...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Question 2: Grandma walks 3 miles every day on her favorite walking trail, which includes 2 miles of walking on the beach and 1 mile of walking on the sidewalk.  On the sidewalk, Grandma walks at twice the rate of speed that she does on the beach.  If 40 minutes of her walk is spent on the beach, how long does it take for her to complete the entire 3-mile walk, in minutes?\n",
            "\n",
            "Correct answer: 50\n",
            "\n",
            "Failed to follow formatting instructions. Full generated text:  To solve this problem, we need to find the time it takes for Grandma to walk the entire 3-mile trail. We know that she walks 3 miles in total, and we can use the information given to find the time it takes for her to walk each segment of the trail.\n",
            "\n",
            "First, let's find the time it takes for her to walk the beach segment. We know that she walks at twice the rate of speed on the sidewalk, so we can set up the following equation:\n",
            "\n",
            "Time on beach = Distance on beach / Speed on sidewalk\n",
            "\n",
            "Time on beach = 2 miles / 2 miles per hour\n",
            "...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Format rate: 0.00% (0/3)\n",
            "Accuracy: 0.00% (0/0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "evaluate_gsm8k(model, tokenizer, val_data, verbose=True, num_eval=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad1bbb5a-d395-4595-91b1-1fa98f1e4ac8",
      "metadata": {
        "id": "ad1bbb5a-d395-4595-91b1-1fa98f1e4ac8"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "The `format_gsm8k` function below processes a data point (question and answer) to provide instructions and formatting in the prompt and to tokenize and combine the inputs in preparation for SFT. Run the code and observe the example results, then answer the question below. You do not need to modify the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "003116f9-22ff-4555-9f35-49b88cc979f6",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "003116f9-22ff-4555-9f35-49b88cc979f6",
        "outputId": "29f8f922-fcca-4e20-ac3e-0477ba72c71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Question Tokenized: {'input_ids': [818, 3478, 812, 11, 314, 1183, 307, 5403, 616, 3956, 338, 2479, 13, 383, 2160, 286, 674, 9337, 481, 788, 307, 4153, 812, 1468, 13, 1374, 1468, 716, 314, 783, 30], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "\n",
            "First Answer Tokenized: {'input_ids': [5756, 1395, 307, 616, 2479, 783, 13, 198, 818, 3478, 812, 11, 314, 1183, 307, 1395, 10, 940, 812, 1468, 13, 198, 818, 3478, 812, 11, 616, 3956, 481, 307, 357, 55, 10, 940, 27493, 16, 14, 17, 812, 1468, 13, 198, 818, 3478, 812, 11, 357, 55, 10, 940, 8, 1343, 357, 55, 10, 940, 27493, 16, 14, 17, 796, 4153, 13, 198, 2396, 357, 55, 10, 940, 27493, 18, 14, 17, 796, 4153, 13, 198, 55, 10, 940, 796, 4153, 1635, 362, 14, 18, 796, 1542, 13, 198, 55, 796, 1542, 532, 838, 796, 1160, 812, 1468, 13, 198, 4242, 1160], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "\n",
            "Combined tokenized input: [50, 6442, 428, 10688, 1917, 13, 5438, 534, 670, 290, 1234, 534, 2457, 3280, 319, 262, 938, 1627, 355, 25, 1303, 21017, 685, 41484, 60, 198, 198, 24361, 25, 554, 3478, 812, 11, 314, 1183, 307, 5403, 616, 3956, 338, 2479, 13, 383, 2160, 286, 674, 9337, 481, 788, 307, 4153, 812, 1468, 13, 1374, 1468, 716, 314, 783, 30, 198, 33706, 25, 220, 5756, 1395, 307, 616, 2479, 783, 13, 198, 818, 3478, 812, 11, 314, 1183, 307, 1395, 10, 940, 812, 1468, 13, 198, 818, 3478, 812, 11, 616, 3956, 481, 307, 357, 55, 10, 940, 27493, 16, 14, 17, 812, 1468, 13, 198, 818, 3478, 812, 11, 357, 55, 10, 940, 8, 1343, 357, 55, 10, 940, 27493, 16, 14, 17, 796, 4153, 13, 198, 2396, 357, 55, 10, 940, 27493, 18, 14, 17, 796, 4153, 13, 198, 55, 10, 940, 796, 4153, 1635, 362, 14, 18, 796, 1542, 13, 198, 55, 796, 1542, 532, 838, 796, 1160, 812, 1468, 13, 198, 4242, 1160]\n",
            "\n",
            "Prediction targets/labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5756, 1395, 307, 616, 2479, 783, 13, 198, 818, 3478, 812, 11, 314, 1183, 307, 1395, 10, 940, 812, 1468, 13, 198, 818, 3478, 812, 11, 616, 3956, 481, 307, 357, 55, 10, 940, 27493, 16, 14, 17, 812, 1468, 13, 198, 818, 3478, 812, 11, 357, 55, 10, 940, 8, 1343, 357, 55, 10, 940, 27493, 16, 14, 17, 796, 4153, 13, 198, 2396, 357, 55, 10, 940, 27493, 18, 14, 17, 796, 4153, 13, 198, 55, 10, 940, 796, 4153, 1635, 362, 14, 18, 796, 1542, 13, 198, 55, 796, 1542, 532, 838, 796, 1160, 812, 1468, 13, 198, 4242, 1160]\n"
          ]
        }
      ],
      "source": [
        "def format_gsm8k(example, tokenizer):\n",
        "    \"\"\"Format example for training with proper label masking.\n",
        "\n",
        "    Args:\n",
        "        example: Dictionary with 'question' and 'answer' keys\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with 'input_ids', 'attention_mask', and 'labels'\n",
        "        Labels are -100 for question tokens (ignored in loss) and\n",
        "        actual token IDs for answer tokens.\n",
        "    \"\"\"\n",
        "    question_text = f\"\"\"Solve this math problem. Show your work and put your final answer on the last line as: #### [answer]\n",
        "\n",
        "Question: {example['question']}\n",
        "Answer: \"\"\"\n",
        "\n",
        "    answer_text = f\"{example['answer']}\"\n",
        "\n",
        "    # Tokenize question and answer separately\n",
        "    question_tokens = tokenizer(question_text, add_special_tokens=True)\n",
        "    answer_tokens = tokenizer(answer_text, add_special_tokens=False)\n",
        "\n",
        "    # Combine\n",
        "    input_ids = question_tokens['input_ids'] + answer_tokens['input_ids']\n",
        "    attention_mask = question_tokens['attention_mask'] + answer_tokens['attention_mask']\n",
        "\n",
        "    # Create labels: -100 for question (ignored), actual tokens for answer\n",
        "    labels = [-100] * len(question_tokens['input_ids']) + answer_tokens['input_ids']\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "# Example:\n",
        "print(f\"First Question Tokenized: {tokenizer(train_data[0]['question'], add_special_tokens=True)}\")\n",
        "print()\n",
        "print(f\"First Answer Tokenized: {tokenizer(train_data[0]['answer'], add_special_tokens=True)}\")\n",
        "print()\n",
        "example_formatted = format_gsm8k(train_data[0], tokenizer)\n",
        "print(f\"Combined tokenized input: {example_formatted['input_ids']}\")\n",
        "print()\n",
        "print(f\"Prediction targets/labels: {example_formatted['labels']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2fa0041-7070-4173-99b2-568403a74bc3",
      "metadata": {
        "id": "a2fa0041-7070-4173-99b2-568403a74bc3"
      },
      "source": [
        "Note that the `-100` label is used for the question part of the sequence. `-100` is the default `ignore_index` for [PyTorch's CrossEntropyLoss](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) implementation, meaning predictions at these points are ignored when calculating the loss (and any backpropagation/derivative calculations).\n",
        "\n",
        "**Task 1 Question.** In one to two paragraphs, explain why we ignore these predictions when instruction tuning with SFT on this question and answer task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10375b21-a928-4668-ae80-f67c84070f96",
      "metadata": {
        "id": "10375b21-a928-4668-ae80-f67c84070f96"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "So with SFT, the goal here is that we want our model to use the question as context so that it can learn how to generate correct responses instead of predicting the inputs or questions. Lets say that if we do include the question tokens in the loss, then that would mean that the model would be rewarded for copying the input instead of just learning the reasonings steps to produce the final answer. So by setting the question tokens' labels to -100, we are just simply telling the model to just ignore the question tokens during training so that no gradients are calculated for those question tokens. So basically by masking with -100, we are excluding those question tokens from the loss computation and this ensures that the loss function only considered answer tokens. With this, it allows the model to focus more on generating step-by-step reasoning and correctly formatted answers instead of memorizing or reproducing the question."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd60615-2d8c-4bed-a743-d11f77db073a",
      "metadata": {
        "id": "afd60615-2d8c-4bed-a743-d11f77db073a"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Now we are ready to fine-tune our model. We will use the high-level [Hugging Face `Trainer` API](https://huggingface.co/docs/transformers/en/main_classes/trainer), backended by PyTorch, to streamline some of the boilerplate training code.\n",
        "\n",
        "Run the following code cell to define the training procedure. You do not need to modify this code, but you should review it briefly before proceeding. Some highlights to consider:\n",
        "\n",
        "1. The top-level `train_gsm8k` function is what you will call later to fine-tune your model. Observe that it has a large number of parameters that you will need to set.\n",
        "\n",
        "2. LoRA (Low Rank Adaptation) is implemented for the attention layers of the network using `lora_r` for the rank and `lora_drop` for the droprate of dropout to be used.\n",
        "\n",
        "3. The data are tokenized with the `format_gsm8k` function defined and explored above.\n",
        "\n",
        "4. The Training arguments and Trainer instantiation apply the many remaining parameter selections for running the Adam optimizer with gradient accumulation and early stopping.\n",
        "  - Gradient accumulation is a way to increase the effective batch size without increasing the memory overhead of training. This is important because you may run out of memory on the GPU in which case training cannot proceed. A gradient step will only be taken after `gradient_accumulation_steps` many minibatches of `batch_size` have been processed -- adding these gradients together rather than resetting after each batch. The effective batch size becomes `gradient_accumulation_steps * batch_size` while the memory necessary scales more closely with `batch_size`.\n",
        "  - We have discussed early stopping at length and used it before -- the logic is implemented by the `Trainer` but you will need to specify the `patience` hyperparameter as well as a `max_epochs` if the early stopping condition is not reached. The code measures validation loss on the held out validation set once per epoch as the performance measure for implementing early stoppping.\n",
        "  - Finally, as always the `learning_rate` must be set.\n",
        "  \n",
        "5. During training, every `logging_steps = 20` gradient steps, the training loss will be printed. After each epoch of training, train and validation losses will be printed. Once training completes, the `evaluate_gsm8k` function is run on the held out test set to measure final performance in terms of following the formatting instructions and correctness of the final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ffbe7875-f135-48f9-8f19-cc1906480dea",
      "metadata": {
        "tags": [],
        "id": "ffbe7875-f135-48f9-8f19-cc1906480dea"
      },
      "outputs": [],
      "source": [
        "# You do not need to modify this code\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, TrainerCallback, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "# Define a simple callback to print the train loss periodically\n",
        "class PrintLossCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None and 'loss' in logs:\n",
        "            print(f\"Step {state.global_step}: Train loss={logs['loss']:.4f}\")\n",
        "\n",
        "def train_gsm8k(model, tokenizer, train_data, val_data, test_data,\n",
        "                batch_size, gradient_accumulation_steps,\n",
        "                learning_rate, max_epochs, patience,\n",
        "                lora_r, lora_drop):\n",
        "    \"\"\"Fine-tune model with LoRA on GSM8K.\"\"\"\n",
        "\n",
        "    # Set padding token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Apply LoRA\n",
        "    lora_config = LoraConfig(r=lora_r, lora_alpha=lora_r * 2,\n",
        "                             target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "                             lora_dropout=lora_drop, bias=\"none\",\n",
        "                             task_type=TaskType.CAUSAL_LM)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # Format and tokenize with proper masking\n",
        "    train_tokenized = train_data.map(\n",
        "        lambda x: format_gsm8k(x, tokenizer),\n",
        "        remove_columns=train_data.column_names\n",
        "    )\n",
        "    val_tokenized = val_data.map(\n",
        "        lambda x: format_gsm8k(x, tokenizer),\n",
        "        remove_columns=val_data.column_names\n",
        "    )\n",
        "\n",
        "    # Data collator for padding with label padding\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=-100, padding=True)\n",
        "\n",
        "    # Training arguments\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./gsm8k_checkpoints\",\n",
        "        num_train_epochs=max_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        logging_steps=20,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_tokenized,\n",
        "        eval_dataset=val_tokenized,\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience), PrintLossCallback()],\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\nFinal test results:\")\n",
        "    evaluate_gsm8k(model, tokenizer, test_data)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa352c9d-e49c-4f27-b256-7ff924b818bb",
      "metadata": {
        "id": "fa352c9d-e49c-4f27-b256-7ff924b818bb"
      },
      "source": [
        "**TODO** Fine-tune the model using the training procedure defined above. Your goal is to select appropriate hyperparameters to achieve:\n",
        "- **Format rate > 90%**: The model should follow instructions to put the final answer after ####\n",
        "- **Accuracy > 30%**: Among properly formatted responses, at least 30% should have the correct answer\n",
        "\n",
        "**Hyperparameters to set:**\n",
        "- `batch_size`: Size of each training minibatch (try 2, 4, or 8)\n",
        "- `gradient_accumulation_steps`: Number of batches to accumulate before updating weights (try 4 or 8)\n",
        "- `learning_rate`: Step size for optimization (try values between 1e-5 and 1e-3)\n",
        "- `max_epochs`: Maximum training epochs (try 5-10)\n",
        "- `patience`: Early stopping patience in epochs (try 2 or 3)\n",
        "- `lora_r`: Rank for LoRA adaptation (try 8, 16, or 32)\n",
        "- `lora_drop`: Dropout rate for LoRA layers (try 0.05 or 0.1)\n",
        "\n",
        "**Tips and hints:**\n",
        "- If you encounter CUDA out of memory errors, reduce `batch_size` or increase `gradient_accumulation_steps`\n",
        "- The effective batch size is `batch_size * gradient_accumulation_steps`\n",
        "- Training should take 10-30 minutes depending on your hyperparameters (assuming GPU/cuda)\n",
        "- You may need to experiment with multiple configurations to achieve the target performance\n",
        "\n",
        "**Strategy:** Start with conservative values (small batch size, moderate learning rate, higher LoRA rank) and adjust based on results. Monitor the training loss - it should decrease steadily. If validation loss stops improving or increases while training loss continues decreasing, you may be overfitting.\n",
        "\n",
        "Fill in the hyperparameters below and run the training. When you have achieved the minimum format rate and accuracy thresholds, answer the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "543d0df3-e3cd-4db7-b74a-3282126e6de8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "543d0df3-e3cd-4db7-b74a-3282126e6de8",
        "outputId": "cfc3e306-65aa-478f-8def-634310454989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fresh model loaded and ready for fine-tuning\n",
            "trainable params: 2,359,296 || all params: 1,420,630,016 || trainable%: 0.1661\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='288' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [288/320 08:36 < 00:57, 0.55 it/s, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.329900</td>\n",
              "      <td>1.015784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.925900</td>\n",
              "      <td>0.924909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.910900</td>\n",
              "      <td>0.910676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.843900</td>\n",
              "      <td>0.902869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.825900</td>\n",
              "      <td>0.899561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.794900</td>\n",
              "      <td>0.899252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.790100</td>\n",
              "      <td>0.900425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.750700</td>\n",
              "      <td>0.901418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.738400</td>\n",
              "      <td>0.902062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 20: Train loss=1.3299\n",
            "Step 40: Train loss=1.0178\n",
            "Step 60: Train loss=0.9259\n",
            "Step 80: Train loss=0.9109\n",
            "Step 100: Train loss=0.8397\n",
            "Step 120: Train loss=0.8439\n",
            "Step 140: Train loss=0.8118\n",
            "Step 160: Train loss=0.8259\n",
            "Step 180: Train loss=0.7949\n",
            "Step 200: Train loss=0.7788\n",
            "Step 220: Train loss=0.7901\n",
            "Step 240: Train loss=0.7507\n",
            "Step 260: Train loss=0.7821\n",
            "Step 280: Train loss=0.7384\n",
            "\n",
            "Final test results:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [04:07<00:00,  4.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Format rate: 92.00% (46/50)\n",
            "Accuracy: 30.43% (14/46)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Set your hyperparameters here\n",
        "\n",
        "batch_size = 2  # Try 2, 4, or 8\n",
        "gradient_accumulation_steps = 8  # Try 4 or 8\n",
        "learning_rate = 1e-4  # Try 1e-4, 5e-5, or 1e-5\n",
        "max_epochs = 10  # Try 5-10\n",
        "patience = 3  # Try 2 or 3\n",
        "lora_r = 8  # Try 8, 16, or 32\n",
        "lora_drop = 0.05  # Try 0.05 or 0.1\n",
        "\n",
        "# Free GPU memory and reload a fresh model for fine-tuning\n",
        "# This is to ensure that if you run this cell multiple times\n",
        "# with different hyperparameters, you always start from the\n",
        "# clean original pretrained model (not a partially fine-tuned one)\n",
        "import gc\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-1_5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Fresh model loaded and ready for fine-tuning\")\n",
        "\n",
        "# Run training\n",
        "model = train_gsm8k(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_data=train_data,\n",
        "    val_data=val_data,\n",
        "    test_data=test_data,\n",
        "    batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_epochs=max_epochs,\n",
        "    patience=patience,\n",
        "    lora_r=lora_r,\n",
        "    lora_drop=lora_drop\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01024252",
      "metadata": {
        "id": "01024252"
      },
      "source": [
        "**Answers to post-training questions:**\n",
        "\n",
        "**Q1.** Did you observe model overfitting? If so, what did you change to mitigate the overfitting?\n",
        "\n",
        "**A1.** From the table, I find that there is a slight overfitting that has occurred since the training loss keeps going down while the validation loss stopped improving after a few epochs. My current apporach to deal with overfitting is to use early stopping, adding dropout, and try to keep the LoRA rank small in order to control the model complexity. With this approach, it does help it bit to keep the validation loss stable and to prevent the model form just memorizing the data. But other ways that I could have done to avoid overfitting would be to lower the learning rate or reducing the number of training epochs.\n",
        "\n",
        "**Q2.** Did you have difficulty with slow training or running out of memory? If so, what changes did you make to address these problems?\n",
        "\n",
        "**A2.** During this experiment with testing out the hyperparameters, there was this one time where I did encountered a CUDA out-of-memory error. With my original approach, my batch size was 8 and my gradient accumulation steps was 4, but I ran out of memory when I start running the code. So to fix this error, I reduced the batch size to 2 and increased the gradient accumulation steps to 8 which does resolved this issue. With this change, it made training more stable and requires less GPU memory, but it also made each training step to be slower since the gradients are updated less fequently. So ultimately, the training does take a longer time to finish, but it still ran smoothly without crashing or experiencing any errors with the changes being made.\n",
        "\n",
        "**Q3.** Based on the results, what is more challenging: (i) Training a language model to follow formatting instructions, or (ii) reason mathematically?\n",
        "\n",
        "**A3.** For the most part, I would say that reasoning mathematically is much harder than following formatting instructions. From my result at least, the model is easily able to learn the output format with a format rate of 92%, but the model did struggled more with solving the problems correctly since it has an accuracy of around 30%. The reasoning behind this is that formatting is mainly about following patterns which is something a model could easily do. On the other hand, reasoning does require multiple logic steps and number calculations which are simply more complex for the model to learn."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}