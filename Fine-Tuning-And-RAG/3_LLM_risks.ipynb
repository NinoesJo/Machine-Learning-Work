{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuu3dLhAON6C"
      },
      "source": [
        "# Part 3: LLM Risks\n",
        "\n",
        "There is no programming in this part of the assignment. Instead, this part is about reading an influential paper on the possible risks and dangers of large language model training and deployment from a human and societal perspective. You will respond briefly to several reflection questions.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "Through this exercise, you will:\n",
        "1. **Read** about the possible societal risks of large language models.\n",
        "2. **Interpret** these risks and **articulate** your own view.\n",
        "\n",
        "To begin, read the 2021 article [*On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?*](https://dl.acm.org/doi/10.1145/3442188.3445922) by Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Schmitchell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLoY0i08PYDw"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Referencing the article but in your own words, articulate **at least two** societal challenges or risks assosciated with large language models. Explain in 1-2 paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNpknHZ8BRW"
      },
      "source": [
        "From reading the article, some of the challenges or risks that are associated with large language models is their environmental and financial cost. The problem here is that training models tend to require tons of amount of energy in which it produces large carbon emissions and contributes to climate change. From the article, it did mentioned that marginalized groups tend to bear most of the enviornmental harm from these technologies even though these technologies rarely benefit these communities. Also the high financial cost limits NLP research developlment to well-funded companies and institutions, in which it leads to inequality in who can develop and influence AI technologies.\n",
        "\n",
        "Another challenge that was brought up is the bias and harm that can be found from training data. Since LLMs are typically trained on huge amounts of online data, they tend to learn and reinforce harmful societal biases that can be found in these data such as racism and sexism. With this, it can lead to outputs that are discrimintory or outputs the spread misinformation. The article also emphasizes that having a bigger dataset does not mean that the data is diverse or lead to greater representation, but instead that bigger datasets would often prioritize powerful groups while excluding marginalized perspectivies in which it would run a risk of worsening social disparities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEPhMANd8BRX"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "In Section 7, the article discusses Paths Forward. Briefly summarize the authors' suggestions. Do you believe these suggestions can mitigate the risks you identified in task 1? Why or why not? Explain in 1-2 paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og8dGvN-8BRX"
      },
      "source": [
        "From section 7, this section does suggest some ways that can help to reduce the risks that LLMs poses such as improving how data is collected and documented, as well as developing models that can prioritize energy efficiency and social responsibility. The authors of this article really encourages researchers to plan ahead before building large-scale systems, take into account with the environmental costs, and to focus on creating smaller and well-organized datasets instead of collecting huge amounts of online data. The authors here also recommended using value sensitive design in which it works directly with affected communities in order to ensure that AI systems does support fairness and inclusivity.\n",
        "\n",
        "In my opinion, I do beleive that these suggestions could help address the risks that I mentioned from task 1. With good data documentation and active stakeholder invovlement, I say that this can help limit and prevent bias by promoting diverse representation and keeping AI systems fair. Also, focusing on energy efficency and smaller models would help cut carbon emissions and can make AI development more accessible to companies and institutions that are less-resourced to address disparities in opportunity to contribute to AI development. These solutions that the authors brought up does seem that it would require more time and collaboration, I say that this can head towards a realistic path towards making these technologies more fair and sustainable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7raP0GF8BRX"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "Since the publication of this paper, some scientists have [called for a pause](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) on further development and research on large language models. Take a position (agree or disagree) on the following claim: \"Large language model development is advancing too rapidly and the risks of such rapid development outweigh the short-term benefits.\" Explain why you hold your position in 1-2 paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyavXKOpQyFX"
      },
      "source": [
        "In my honest opinion, I would argue with the claim that large language model development is advancing too rapidly and that the risks do outweigh the short-term benefits. I say that the pace of innovation with AI development is currently advancing faster than society can manage or understand these AI systems in which would lead to major environmental, social, and ethical risks.Going back to the article, training very large models would consumes enormous amount of energy which would harm the environment and gives more control to big tech companies that can afford massive computing and energy costs. With this, it would increase the inequality even more by limiting control andf benefits of AI to a small group.\n",
        "\n",
        "Another thing I want to point out is that scaling AI rapidly without any strong regulation put into place would increase the risk of producing biased or misleading outputs. From what I learned from the article, I find models would often reproduce stereotypes, would spread misinformation, and these models would rely on hidden processes that are hard for people who built these models to understand and interpret. So ultimately, pausing rapid AI development would not be a bad idea as it would allow researchers to focus more on making AI more transparent, more safe, and more fair so that AI models can better serve for the public and society."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}