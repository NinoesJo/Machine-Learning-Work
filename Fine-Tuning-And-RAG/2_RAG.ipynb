{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfL-oaGagbq2"
   },
   "source": [
    "# Part 2: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this part of the assignment, you will implement a Retrieval Augmented Generation (RAG) system that combines information retrieval with text generation. RAG systems address a key limitation of language models: their inability to access information beyond their training data. By retrieving relevant documents from a knowledge base and incorporating them into the generation process, RAG systems can provide more accurate, up-to-date, and factually grounded responses.\n",
    "\n",
    "## Learning Objectives\n",
    "You will:\n",
    "1. Build a question-answering system using the [SQUAD Dataset](https://rajpurkar.github.io/SQuAD-explorer/) consisting of natural language questions, answers, and contexts.\n",
    "2. Prompt a Phi-2 causal Transformer Language Model to generate answers\n",
    "3. Use an encoder Transformer language model to compute dense embeddings of a dataset of contexts drawn from wikipedia articles\n",
    "4. Implement RAG by embedding a query vector, searching for and rerieving relevant contexts, and providing context to the Phi-2 model to improve generated answers\n",
    "5. Evaluate the performance of question answering system with and without RAG for accuracy and efficiency\n",
    "\n",
    "Note: This assignment is intended to utilize GPU resources such as `CUDA` through the CS department cluster, Google colab (or local GPU resources for those running on machines with GPU support). The **code below assumes CUDA**; you will need to modify it if working with the [`mps` backend](https://docs.pytorch.org/docs/stable/notes/mps.html).\n",
    "\n",
    "To start, run the following code to download the Phi-2 model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222,
     "referenced_widgets": [
      "26039e5507bd461fba0a7da9f8bea465",
      "4a5f227c5684426891c7e5f3414d1b67",
      "dd69aeb8ca6c4a19b4ab037d27f18746",
      "ea1f8a63fa89419f9508d887dd6cf6d1",
      "436bd4034fd0403c861cabaa5bbfb00d",
      "d5610042d8cd4621b6de9e4874abbfd9",
      "89e17778e2e04dd79d1094c6e2c0a786",
      "da0af3e63b4b4f2bb0fdb21e24189bef",
      "758868075e884aef92014f81fdb0eff9",
      "83659b0dcb7e4dfaa47fb2d78ef34c21",
      "eff498f319934ce28544c02f63f57af6"
     ]
    },
    "id": "hTsrehw6mgfm",
    "outputId": "e1a6ac87-56ba-4cad-b437-611c3244aa14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26039e5507bd461fba0a7da9f8bea465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Download Successful\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Model Download Successful\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iedZ_iLjGaX"
   },
   "source": [
    "Now run the following to demonstrate basic generation with the model. Note that this code snippet uses sampling with temperature for generation -- you can run the code cell multiple times and get different short stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpWROx_vjC9S",
    "outputId": "333a72fb-f1e8-446c-8da2-43c97b5ef3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example model generation:\n",
      "========================\n",
      "Once upon a time, in the small town of Oakville, there lived a young man named John. He was known for his extraordinary intelligence and his passion for environmental science. John had always been fascinated by the intricate web of nutrient cycling and how it affected the delicate balance of ecosystems.\n",
      "\n",
      "John's day began like any other. He woke up early in the morning, eager to delve into his research on atmospheric oxygen production. After a quick breakfast, he made his way to the laboratory where he spent countless hours studying\n"
     ]
    }
   ],
   "source": [
    "print(\"Example model generation:\")\n",
    "print(\"========================\")\n",
    "inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100,\n",
    "                         pad_token_id=tokenizer.eos_token_id,\n",
    "                         do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HotcjZDnjsTH"
   },
   "source": [
    "Run the following to download the [SQUAD Dataset](https://rajpurkar.github.io/SQuAD-explorer/). We take the `validation` split consisting of just over 10,000 examples, each with a question, an answer, and a a *context*: This is a short passage from a Wikipedia article in which the answer can be found. We use the smaller validation set for efficiency, and note that we will not be doing any model training in this assignment part, only retrieval augmented generation with the pretrained Phi-2 model.\n",
    "\n",
    "After downloading the dataset, the code prints a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuYw6mEjoPUh",
    "outputId": "10440ed5-2af2-415f-8fac-e9d4ee61a345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download Successful\n",
      "\n",
      "\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "\n",
      "Ground truth answer: Denver Broncos\n",
      "\n",
      "Context containing the answer: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load SQuAD dataset\n",
    "squad = load_dataset(\"squad\", split=\"validation\")\n",
    "print(\"\\nDownload Successful\\n\\n\")\n",
    "\n",
    "# Get one example\n",
    "example = squad[0]\n",
    "question = example[\"question\"]\n",
    "context = example[\"context\"]\n",
    "answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"\\nGround truth answer:\", answer)\n",
    "print(\"\\nContext containing the answer:\", context)\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUHOY0FtlANZ"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "First we need to discuss prompt engineering. Phi-2 has a tendency (by default, from its pretraining) to continue generating long sequences well beyond the immediate question or task posed in the prompt. Indeed, often it will continue asking and answering additional lists of questions.\n",
    "\n",
    "However, Phi-2 has also been instruction tuned. Following the instruction-tuning prompt format helps the model to better follow the specific intent of a user query.\n",
    "\n",
    "The code below loops through three different example questions. For each, generate two different responses from the Phi-2 model:\n",
    "- One using the question itself as the only prompt/input to the model\n",
    "- Another using the instruction format from the [Phi-2 Documentation](https://huggingface.co/microsoft/phi-2). Specifically, the prompt/input to the model should be `f\"Instruct: {question}\\nOutput:\"`.\n",
    "\n",
    "Then briefly explain the qualitative differences you observe. Describe how instruction-tuning using SFT can give rise to the differences you observe. Answer in 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SP1vRhfUlg1a",
    "outputId": "000aecaa-83e5-40d8-8f05-1ccefc9fad7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Question 0 : Which NFL team represented the AFC at Super Bowl 50?\n",
      "\n",
      "==================================================\n",
      "Basic Prompting:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which NFL team represented the AFC at Super Bowl 50?\n",
      "Answer: The Denver Broncos.\n",
      "\n",
      "Exercise: Who was the head coach of the Denver Broncos during Super Bowl 50?\n",
      "Answer: John Fox.\n",
      "\n",
      "Exercise: How many times had the Denver Broncos appeared in a Super Bowl before Super Bowl 50?\n",
      "Answer: Four times.\n",
      "\n",
      "Exercise: Who was the quarterback for the Denver Broncos during Super Bowl 50?\n",
      "Answer: Peyton Manning.\n",
      "\n",
      "Exercise: Who was the head coach of the Carolina Panthers during Super Bowl 50?\n",
      "Answer: Ron Rivera.\n",
      "\n",
      "Exercise: How many times had the Carolina Panthers appeared in a Super Bowl before Super\n",
      "\n",
      "==================================================\n",
      "Instruction Prompting:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Output: The Denver Broncos represented the AFC at Super Bowl 50.\n",
      "\n",
      "==================================================\n",
      "Question 1000 : Where is a palm house with subtropic plants from all over the world on display?\n",
      "\n",
      "==================================================\n",
      "Basic Prompting:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is a palm house with subtropic plants from all over the world on display?\n",
      "\n",
      "Answer: The Palm House at Kew Gardens in London.\n",
      "\n",
      "Exercise 3:\n",
      "How did the Palm House at Kew Gardens get its name?\n",
      "\n",
      "Answer: It was named after the palm trees that were originally planted in the greenhouse.\n",
      "\n",
      "Exercise 4:\n",
      "What is the purpose of the Palm House at Kew Gardens?\n",
      "\n",
      "Answer: It is a place for people to see and learn about different types of palm trees from all over the world.\n",
      "\n",
      "Exercise 5:\n",
      "Why is the Palm House at Kew Gardens important?\n",
      "\n",
      "Answer: It is important because it is a\n",
      "\n",
      "==================================================\n",
      "Instruction Prompting:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Where is a palm house with subtropic plants from all over the world on display?\n",
      "Output: The Palm House.\n",
      "\n",
      "==================================================\n",
      "Question 2000 : What is dramatic gesturing an example of?\n",
      "\n",
      "==================================================\n",
      "Basic Prompting:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is dramatic gesturing an example of?\n",
      "Answer: Dramatic gesturing is an example of nonverbal communication.\n",
      "\n",
      "Exercise 3:\n",
      "What is the purpose of using a microphone in a presentation?\n",
      "Answer: The purpose of using a microphone in a presentation is to amplify the speaker's voice so that everyone in the audience can hear them clearly.\n",
      "\n",
      "Exercise 4:\n",
      "What is the purpose of using a projector in a presentation?\n",
      "Answer: The purpose of using a projector in a presentation is to display visual aids such as slides or images on a screen for the audience to see.\n",
      "\n",
      "Exercise 5:\n",
      "What is the purpose of using a\n",
      "\n",
      "==================================================\n",
      "Instruction Prompting:\n",
      "\n",
      "Instruct: What is dramatic gesturing an example of?\n",
      "Output: What is an example of dramatic gesturing?\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 1000, 2000]:\n",
    "  example = squad[i]\n",
    "  question = example[\"question\"]\n",
    "\n",
    "  print(\"\\n\" + \"=\"*50)\n",
    "  print(\"Question\", i, \":\", question)\n",
    "  print(\"\\n\" + \"=\"*50)\n",
    "  print(\"Basic Prompting:\\n\")\n",
    "  # TODO: Generate and print answer to question\n",
    "  # using basic prompting of just the question\n",
    "  input_1 = tokenizer(question, return_tensors = \"pt\").to(\"cuda\")\n",
    "  output_1 = model.generate(**input_1, max_new_tokens = 128)\n",
    "  print(tokenizer.decode(output_1[0], skip_special_tokens = True).strip())\n",
    "\n",
    "  print(\"\\n\" + \"=\"*50)\n",
    "  print(\"Instruction Prompting:\\n\")\n",
    "  # TODO: Generate and print answer to question\n",
    "  # using instruction prompting of just the question\n",
    "  instruction_prompt = f\"Instruct: {question}\\nOutput:\"\n",
    "  input_2 = tokenizer(instruction_prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "  output_2 = model.generate(**input_2, max_new_tokens = 128)\n",
    "  print(tokenizer.decode(output_2[0], skip_special_tokens = True).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcZ8G4f4oIuM"
   },
   "source": [
    "**TODO**: Briefly explain the qualitative differences you observe. Describe how instruction-tuning using SFT can give rise to the differences you observe. Answer in 1-2 paragraphs.\n",
    "\n",
    "**Your Answer:** Looking at the results, the first thing I tend to notice when I used basic prompting is that it tends to generate longer and less focused responses. Like for a given question, the model does managed to give the correct answer but it also often ask and answer related questions that were not originally asked. What this really shows is that without giving some explicit intructions, the model tend to continue on generating text since it was pretrained on long and open-ended text sequences.\n",
    "\n",
    "On the other hand, I find that with instruction prompting, I notice that it produced much more concise and direct answers to the question. So instead of continuing with extra questions like with the basic prompting, the model is more focused on giving a single repsonse that is well-structured. This is most likely due to the fact that the Phi-2 model was trained using SFT in which it learned from examples that follows the instruction (Instruct/Output) format. With this, the model can understand user instructions more clearly and its responses would be more focused on answering the questions or instructions it was being asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNpO93Zpoohm"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "In this task we will measure an empirical baseline of performance to motivate the implementation of a RAG.\n",
    "\n",
    "Several key utility functions will be provided for you. The first is `get_answer`, defined and documented below. You do not need to edit this code, but you should read and familiarize yourself with the function as you will be using it next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rjEXUt7jtwZ0"
   },
   "outputs": [],
   "source": [
    "def get_answer(model, tokenizer, question, context=None, max_len=50):\n",
    "  \"\"\"\n",
    "    Generate an answer to a question using the language model.\n",
    "    This function constructs a prompt in the instruction-tuned format that\n",
    "    Phi-2 was trained on, generates a response, and returns only the newly\n",
    "    generated tokens (excluding the input prompt). For coherence, uses greedy\n",
    "    decoding, i.e., no sampling or temperature.\n",
    "\n",
    "    Args:\n",
    "        model: The Phi-2 language model\n",
    "        tokenizer: The tokenizer for Phi-2\n",
    "        question (str): The question to answer\n",
    "        context (str, optional): Context passage to help answer the question.\n",
    "                                If None, model answers without context.\n",
    "        max_len (int, optional): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's generated answer, stripped of whitespace\n",
    "    \"\"\"\n",
    "  if context is None:\n",
    "    prompt = f\"Instruct: {question}\\nOutput:\"\n",
    "  else:\n",
    "    prompt = f\"Context: {context}\\nInstruct: {question}\\nOutput:\"\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "  outputs = model.generate(**inputs, max_new_tokens=max_len, pad_token_id=tokenizer.eos_token_id)\n",
    "  outputs = outputs[:, inputs['input_ids'].shape[-1]:] # Take only newly generated tokens\n",
    "  response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rE9RoBqquWZ"
   },
   "source": [
    "First we demonstrate a qualitative examples where providing the relevant context helps the model to answer correctly/truthfully. Run the following code to see three (hand-picked, not random) examples of model answers **with** versus **without** the relevant passage from a wikipedia article provided in context to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rz3RXbj1oRCi",
    "outputId": "cce5f2c9-a66c-492b-dde2-2ed25016e075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who were special guests for the Super Bowl halftime show?\n",
      "\n",
      "Correct Answer: Beyoncé and Bruno Mars\n",
      "\n",
      "Model WITHOUT context:\n",
      "Beyoncé and Jay-Z.\n",
      "\n",
      "Model WITH context:\n",
      "The special guests for the Super Bowl halftime show were Beyoncé and Bruno Mars.\n",
      "\n",
      "==================================================\n",
      "Question: What was media day called for Super Bowl 50?\n",
      "\n",
      "Correct Answer: Super Bowl Opening Night.\n",
      "\n",
      "Model WITHOUT context:\n",
      "What was the name of the media day for Super Bowl 50?\n",
      "\n",
      "Model WITH context:\n",
      "Super Bowl 50 media day was called Super Bowl Opening Night.\n",
      "\n",
      "==================================================\n",
      "Question: When was Warsaw ranked as the 32nd most liveable city in the world?\n",
      "\n",
      "Correct Answer: 2012\n",
      "\n",
      "Model WITHOUT context:\n",
      "In 2015.\n",
      "\n",
      "Model WITH context:\n",
      "Warsaw was ranked as the 32nd most liveable city in the world in 2012.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in [100, 500, 900]:\n",
    "  example = squad[i]\n",
    "  question = example[\"question\"]\n",
    "  context = example[\"context\"]\n",
    "  answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "  print(\"Question:\", question)\n",
    "  print(\"\\nCorrect Answer:\", answer)\n",
    "\n",
    "  print(\"\\nModel WITHOUT context:\")\n",
    "  print(get_answer(model, tokenizer, question))\n",
    "\n",
    "  # Test 2: With context\n",
    "\n",
    "  print(\"\\nModel WITH context:\")\n",
    "  print(get_answer(model, tokenizer, question, context))\n",
    "\n",
    "  print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T86N7HlCrnGp"
   },
   "source": [
    "In the generations above, you should observe that:\n",
    "\n",
    "- Even without context, the model correctly identifies that Beyoncé performed, but it goes on to **hallucinate** that Jay-Z performed with Beyoncé, which is incorrect.\n",
    "\n",
    "- In the second example, without context, the model reverts to a pretraining behavior of just rephrasing the question, whereas with context it correctly restates the question **and** answers it.\n",
    "\n",
    "- In the third example, without context, again the model hallucinates and gives a reasonable sounding but incorrect answer.\n",
    "\n",
    "Now let us evaluate model performance **quantitatively** with and without context provided. To do so, the `evaluate_answer` function is defined and documented for you below. You do not need to modify it but you should review the function which is used to evaluate the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PxvqVriRo5qN"
   },
   "outputs": [],
   "source": [
    "def evaluate_answer(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluate if a predicted answer matches the ground truth answer.\n",
    "\n",
    "    This function uses flexible string matching to handle various answer formats:\n",
    "    - Exact matches\n",
    "    - Ground truth contained in prediction (e.g., \"Paris\" in \"The capital is Paris\")\n",
    "    - Prediction contained in ground truth (e.g., \"Broncos\" matches \"Denver Broncos\")\n",
    "    - Order-independent word matching (e.g., \"Bruno Mars and Beyoncé\" matches\n",
    "      \"Beyoncé and Bruno Mars\")\n",
    "\n",
    "    Args:\n",
    "        predicted (str): The model's predicted answer\n",
    "        ground_truth (str): The correct answer from the dataset\n",
    "\n",
    "    Returns:\n",
    "        int: 1 if the answer is considered correct, 0 otherwise\n",
    "\n",
    "    Note:\n",
    "        This is a simplistic evaluation metric that tends to underestimate\n",
    "        model performance. It is intended for demonstration purposes only.\n",
    "    \"\"\"\n",
    "\n",
    "    pred = predicted.lower().strip()\n",
    "    gt = ground_truth.lower().strip()\n",
    "\n",
    "    # Exact match\n",
    "    if pred == gt:\n",
    "        return 1\n",
    "\n",
    "    # Ground truth contained in prediction\n",
    "    if gt in pred:\n",
    "        return 1\n",
    "\n",
    "    # Prediction contained in ground truth\n",
    "    if pred in gt:\n",
    "        return 1\n",
    "\n",
    "    # Check if all words from ground truth appear in prediction (order-independent)\n",
    "    gt_words = set(gt.split())\n",
    "    pred_words = set(pred.split())\n",
    "    if gt_words.issubset(pred_words):\n",
    "        return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX62buCFydWf"
   },
   "source": [
    "Now that we can evaluate one prediction against the correct answer, the below code uses this as a helper function to evaluate a model on a dataset of questions and answers.\n",
    "\n",
    "Read the function and its documentation, then run two evaluations:\n",
    "\n",
    "- First, run `evaluate_model` using `mode=\"no_context\"` (the default) to assess the model's performance without any context provided beyond the question itself.\n",
    "- Second, run `evaluate_model` using `mode=\"gold_context\"` to assess the model's performance when it is provided with the gold standard context (gold standard in the sense that it contains the correct answer).\n",
    "\n",
    "In both cases, **set `max_examples=100`** to keep things efficient (you could run a longer evaluation if interested, but this is the minimum required for the assignment -- running an evaluation over larger portions of the dataset would take tens of minutes or more).\n",
    "\n",
    "You should see that the model performs quite poorly with no context but substantially better with the gold standard context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cexLQd0wyakG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, mode=\"no_context\",\n",
    "                     context_embeddings=None, contexts=None,\n",
    "                     embedding_model=None, embedding_tokenizer=None,\n",
    "                     max_examples=None):\n",
    "    \"\"\"\n",
    "    Unified evaluation function for comparing different QA approaches.\n",
    "\n",
    "    This function evaluates question-answering performance across three modes:\n",
    "    - \"no_context\": Model answers questions without any context\n",
    "    - \"gold_context\": Model uses the ground-truth context from the dataset\n",
    "    - \"rag\": Model uses retrieved context from the RAG system\n",
    "\n",
    "    For each example, the function:\n",
    "    1. Determines the appropriate context based on mode. mode=\"rag\" requires\n",
    "    a retrieve_top_context implementation along with context_embeddings, contexts,\n",
    "    embedding_model, and embedding_tokenizer.\n",
    "    2. Generates an answer using get_answer()\n",
    "    3. Evaluates correctness using evaluate_answer()\n",
    "    4. Tracks timing for performance analysis\n",
    "\n",
    "    Args:\n",
    "        model: The Phi-2 language model\n",
    "        tokenizer: The tokenizer for Phi-2\n",
    "        dataset: SQUAD dataset containing questions, contexts, and answers\n",
    "        mode (str): Evaluation mode - \"no_context\", \"gold_context\", or \"rag\"\n",
    "        context_embeddings (numpy.ndarray, optional): Precomputed embeddings of all\n",
    "            contexts. Required for mode=\"rag\".\n",
    "        contexts (list, optional): List of all context strings. Required for mode=\"rag\".\n",
    "        embedding_model (optional): Sentence embedding model. Required for mode=\"rag\".\n",
    "        embedding_tokenizer (optional): Tokenizer for embedding model. Required for mode=\"rag\".\n",
    "        max_examples (int, optional): Limit evaluation to first N examples.\n",
    "            If None, evaluates entire dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (accuracy, avg_time, results)\n",
    "            - accuracy (float): Proportion of correct answers (0.0 to 1.0)\n",
    "            - avg_time (float): Average time per question in seconds\n",
    "            - results (list): List of dicts with detailed results for each example\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If mode is not one of the three valid options\n",
    "        ValueError: If mode=\"rag\" but required RAG parameters are None\n",
    "\n",
    "    Note:\n",
    "        - Use max_examples for faster iteration during development\n",
    "        -\n",
    "        - The \"gold_context\" mode represents the upper bound of what's possible\n",
    "          with perfect retrieval (always retrieving context containing the answer)\n",
    "        - Timing includes both retrieval (for RAG) and generation\n",
    "    \"\"\"\n",
    "    correct, total, total_time = 0, 0, 0\n",
    "    results = []\n",
    "\n",
    "    # Limit dataset size if specified\n",
    "    data = dataset if max_examples is None else dataset.select(range(max_examples))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for example in tqdm(data, desc=f\"Evaluating {mode}\"):\n",
    "        question = example[\"question\"]\n",
    "        ground_truth = example[\"answers\"][\"text\"][0]\n",
    "        gold_context = example[\"context\"]\n",
    "\n",
    "        # Determine context based on mode\n",
    "        if mode == \"no_context\":\n",
    "            context = None\n",
    "        elif mode == \"gold_context\":\n",
    "            context = gold_context\n",
    "        elif mode == \"rag\":\n",
    "            context = retrieve_top_context(question, context_embeddings, contexts,\n",
    "                                           embedding_model, embedding_tokenizer)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")\n",
    "\n",
    "        # Get prediction\n",
    "        prediction = get_answer(model, tokenizer, question, context)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Evaluate\n",
    "        score = evaluate_answer(prediction, ground_truth)\n",
    "        correct += score\n",
    "        total += 1\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"prediction\": prediction,\n",
    "            \"correct\": score,\n",
    "            \"time\": elapsed_time\n",
    "        })\n",
    "\n",
    "    total_time += time.time() - start_time\n",
    "    accuracy = correct / total\n",
    "    avg_time = total_time / total\n",
    "\n",
    "    return accuracy, avg_time, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddN8z1M5x31K",
    "outputId": "68dfd4d3-b97b-4415-e495-38d9fe62deb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating no_context: 100%|██████████| 100/100 [00:50<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Context Accuracy: 0.21\n",
      "No Context Average Time: 0.5055790519714356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gold_context: 100%|██████████| 100/100 [01:01<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gold Context Accuracy: 0.87\n",
      "Gold Context Average Time: 0.6156034994125367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run evaluate_model twice:\n",
    "# 1. First with mode=\"no_context\" and max_examples=100\n",
    "# 2. Then with mode=\"gold_context\" and max_examples=100\n",
    "# Print the accuracy and average time for each\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  no_accuracy, no_avg_time, no_results = evaluate_model(model, tokenizer, squad, mode = \"no_context\", max_examples = 100)\n",
    "  print(\"No Context Accuracy:\", no_accuracy)\n",
    "  print(\"No Context Average Time:\", no_avg_time)\n",
    "  print()\n",
    "  gold_accuracy, gold_avg_time, gold_results = evaluate_model(model, tokenizer, squad, mode = \"gold_context\", max_examples = 100)\n",
    "  print()\n",
    "  print(\"Gold Context Accuracy:\", gold_accuracy)\n",
    "  print(\"Gold Context Average Time:\", gold_avg_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnSguTNizk1g"
   },
   "source": [
    "## Task 3\n",
    "\n",
    "Of course, it is not surprising that context helps -- we are providing the answer to the question directly as input to the model, so the attention mechanisms can learn in-context. We would not be able to do this if the SQUAD dataset did not provide the gold standard context (a short text segment from a wikipedia article) containing the answer to each question.\n",
    "\n",
    "But what if we don't have the gold standard context? In real-world applications, we might have a large collection of documents (like Wikipedia articles, company documentation, or research papers) and we need to automatically find the most relevant context for each question. This is where Retrieval Augmented Generation (RAG) comes in.\n",
    "\n",
    "The key idea of RAG is to use an encoder Transformer model (BERT-style) to compute **dense embeddings** to represent both questions and contexts in a shared vector space, where semantically similar texts are close together. We can then use similarity search to find the most relevant context for any given question.\n",
    "\n",
    "In this task, you will first implement the `compute_embeddings` function that takes a list of texts and returns their dense vector representations. We will use a pre-trained sentence transformer model (all-MiniLM-L6-v2) which has been specifically trained to produce embeddings that capture semantic similarity.\n",
    "\n",
    "**Implementation hints for `compute_embeddings`:**\n",
    "- The function should process each text individually (you could extend this to use batching for efficiency, but it's not required)\n",
    "- Use `tokenizer()` to convert text to input tensors, with `padding=True`, `truncation=True`, and `max_length=512`\n",
    "- Pass the tokenized inputs through the model to get outputs\n",
    "- The model returns a complex object - you want `outputs.last_hidden_state`, which has shape (batch_size, sequence_length, hidden_dim)\n",
    "- Use **mean pooling** across the sequence dimension: `.mean(dim=1)` to get a single vector per text by simply averaging the contextual embeddings of each token\n",
    "- You can wrap model inference in `torch.no_grad()` to save memory\n",
    "- Convert the final embeddings to numpy arrays on CPU: `.cpu().numpy()`\n",
    "- Stack all embeddings into a single numpy array using `np.vstack()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ho_5_f2dzVDx"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load a small encoder model for embeddings\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to(\"cuda\")\n",
    "\n",
    "def compute_embeddings(texts, model, tokenizer, batch_size = 32):\n",
    "    \"\"\"\n",
    "    Compute dense embeddings for a list of texts.\n",
    "\n",
    "    TODO: Implement this function. Hints:\n",
    "    - Use model(**inputs) to get outputs\n",
    "    - Use mean pooling: outputs.last_hidden_state.mean(dim=1)\n",
    "    - Remember to use torch.no_grad() and move to CPU\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape (len(texts), embedding_dim)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "      for i in range(0, len(texts), batch_size):\n",
    "        b_texts = texts[i: i + batch_size]\n",
    "        inputs = tokenizer(b_texts, return_tensors = \"pt\", padding = True, truncation = True, max_length = 512).to(\"cuda\")\n",
    "        outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim = 1).cpu().numpy())\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J29jSfAd_lmz"
   },
   "source": [
    "Now that you can compute embeddings, the next step is to implement retrieval. Given a question, you need to:\n",
    "1. Compute its embedding\n",
    "2. Compare it to all context embeddings\n",
    "3. Return the most similar context\n",
    "\n",
    "We'll use **cosine similarity** to measure how similar two embeddings are. Cosine similarity ranges from -1 (opposite) to 1 (identical direction) and is computed as: `dot(A, B) / (||A|| * ||B||)`.\n",
    "\n",
    "**Implementation hints for `retrieve_top_context`:**\n",
    "- Use your `compute_embeddings` function to get the query embedding\n",
    "- Use `np.dot(context_embeddings, query_embedding)` to compute all dot products at once with the optimized `np` (numpy) implementation.\n",
    "- Use `np.linalg.norm(context_embeddings, axis=1)` to compute norms of all context embeddings\n",
    "- Use `np.linalg.norm(query_embedding)` for the query norm\n",
    "- Divide the dot products by the product of norms to get cosine similarities\n",
    "- Use `np.argmax(similarities)` to find the index of the highest similarity\n",
    "- Return the context at that index\n",
    "\n",
    "**Important:** Use numpy's vectorized operations rather than Python for loops for efficiency. This allows you to compare the query against all 10,000+ contexts in milliseconds rather than seconds (which is what will happen if you use Python for loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FN8gT2xz032E"
   },
   "outputs": [],
   "source": [
    "def retrieve_top_context(query, context_embeddings, contexts, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar contexts to query.\n",
    "\n",
    "    TODO: Implement this function. Steps:\n",
    "    1. Compute query embedding using compute_embeddings\n",
    "    2. Calculate cosine similarities with all context embeddings\n",
    "       Formula: dot(A,B) / (norm(A) * norm(B))\n",
    "    3. Return the context whose embedding has the highest similarity score\n",
    "    to the query embedding\n",
    "\n",
    "    Args:\n",
    "        query: str, the query text\n",
    "        context_embeddings: numpy array of context embeddings\n",
    "        contexts: list of context strings\n",
    "        model: embedding model\n",
    "        tokenizer: embedding tokenizer\n",
    "\n",
    "    Returns:\n",
    "        top_context: most similar context string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query_embedding = compute_embeddings([query], model, tokenizer, batch_size = 1)[0]\n",
    "    dot_products = np.dot(context_embeddings, query_embedding)\n",
    "    norm_context_embedding = np.linalg.norm(context_embeddings, axis = 1)\n",
    "    norm_query_embedding = np.linalg.norm(query_embedding)\n",
    "    similarities = dot_products / (norm_context_embedding * norm_query_embedding)\n",
    "    return contexts[np.argmax(similarities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfGDNfyY_r-o"
   },
   "source": [
    "Now test your implementation. The code below (which you can just run, and do not need to edit) will use your above defined `compute_embeddings` and `retrieve_top_context` functions to:\n",
    "1. Extract all contexts from the SQUAD dataset\n",
    "2. Compute embeddings for all ~10,500 contexts (this may take a minute or two)\n",
    "3. Evaluate your retrieval system on 1000 examples (this may take a minute or two, assuming an efficient implementation)\n",
    "\n",
    "**What to expect:**\n",
    "- **Accuracy:** Your retrieval should achieve at least **50%** accuracy at retrieving the gold standard context. This means that for at least half the questions, you successfully retrieve the exact Wikipedia passage that contains the answer. This is actually quite good - remember that there are thousands of possible passages, and many questions could plausibly be answered by multiple passages.\n",
    "  \n",
    "- **Runtime:** Your retrieval should average less than **100ms per query** on a GPU/with CUDA. This time comes from computing the query embedding (forward propagation in the small encoder model) as well as the time for the similarity search itself.\n",
    "\n",
    "If your accuracy is much lower than 50% or your runtime is much slower, double-check your implementation. Common issues include:\n",
    "- Shape mismatches or incorrect averaging for computing the embeddings\n",
    "- Using Python loops instead of numpy vectorized operations\n",
    "- Not using `.to(\"cuda\")` for the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vujcli5W4-VQ",
    "outputId": "07b57f9e-dc73-4a92-9c92-eabb89d4c259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 10570 contexts...\n",
      "Embeddings shape: (10570, 384)\n"
     ]
    }
   ],
   "source": [
    "# Run this code to extract all contexts from the dataset\n",
    "# and compute their embeddings\n",
    "\n",
    "contexts = [example[\"context\"] for example in squad]\n",
    "\n",
    "print(f\"Computing embeddings for {len(contexts)} contexts...\")\n",
    "context_embeddings = compute_embeddings(contexts, embedding_model, embedding_tokenizer)\n",
    "\n",
    "print(f\"Embeddings shape: {context_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJmXC1uB7ZxY",
    "outputId": "55ef9bfa-087a-413f-f4b5-03b47e8ac326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (proportion of retrieving gold standard context): 50.60%\n",
      "Runtime efficiency (avg retrieval time per query): 15.66 ms\n"
     ]
    }
   ],
   "source": [
    "# Run this code to evaluate how effective your embeddings\n",
    "# and retrieval are in terms of accuracy and runtime\n",
    "\n",
    "correct = 0\n",
    "start_time = time.time()\n",
    "\n",
    "max_examples = 1000\n",
    "\n",
    "for i in range(max_examples):\n",
    "    example = squad[i]\n",
    "    query = example['question']\n",
    "    retrieved_context = retrieve_top_context(query, context_embeddings, contexts,\n",
    "                                             embedding_model, embedding_tokenizer)\n",
    "    if retrieved_context == example['context']:\n",
    "        correct += 1\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Accuracy (proportion of retrieving gold standard context): {(correct/max_examples):.2%}\")\n",
    "print(f\"Runtime efficiency (avg retrieval time per query): {(total_time/max_examples)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_Tkbnok4-m_"
   },
   "source": [
    "## Task 4\n",
    "\n",
    "You've now implemented all the components of the RAG system. Now we will evaluate the overall system for question answering. The code below will:\n",
    "1. For each question, retrieve the most similar context using your embeddings\n",
    "2. Provide that retrieved context to the Phi-2 model\n",
    "3. Generate an answer and evaluate its correctness\n",
    "\n",
    "For the sake of efficiency the evaluation is just run on 100 examples.\n",
    "\n",
    "**What to expect:**\n",
    "- **Accuracy:** Your RAG system should achieve **at least 50%** accuracy, substantially better than the 21% without context, though not as good as the 87% with gold standard contexts. This gap is expected - sometimes retrieval finds a relevant but imperfect context, and sometimes it retrieves the wrong passage entirely.\n",
    "\n",
    "- **Runtime:** Total time should average **less than 1 second per query** (using a GPU with cuda).\n",
    "\n",
    "After running the evaluation, **briefly answer the reflection questions that follow in 2-3 sentences each**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QruPORL12Nwm",
    "outputId": "28b6467b-ca52-4fd3-9978-ff24bec58726"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rag: 100%|██████████| 100/100 [00:55<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAG:\n",
      "Accuracy: 64.00%\n",
      "Average time per question: 0.551 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc, avg_time, _ = evaluate_model(model, tokenizer, dataset=squad, mode=\"rag\",\n",
    "                                  context_embeddings=context_embeddings, contexts=contexts,\n",
    "                                  embedding_model=embedding_model, embedding_tokenizer=embedding_tokenizer,\n",
    "                                  max_examples=100)\n",
    "\n",
    "print(\"Using RAG:\")\n",
    "print(f\"Accuracy: {acc:.2%}\")\n",
    "print(f\"Average time per question: {avg_time:.3f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tONme3PFHBUs"
   },
   "source": [
    "**TODO**: Briefly answer these reflection questions in 2-3 sentences each\n",
    "\n",
    "**Q1.** How much does RAG improve over no context in terms of correctness? How much runtime overhead do you observe?\n",
    "\n",
    "**A1.** I find that RAG does significantly increase the model's accuracy from 21% with no context up to 64% with RAG. This result does showcase that giving the model helpful and relevant context does help the model to generate accurate answers. Also, the runtime overhead is quite small since each query takes on average about 0.55 seconds which is well below the 1 second threshold.\n",
    "\n",
    "**Q2.** Under what circumstances would you recommend implementing RAG for a real-world question answering system?\n",
    "\n",
    "**A2.** Pretty much RAG works well when dealing with large data or data that are frequently updated, such as Wikipedia articles, online news, research papers, or company databases to name a few. With RAG, it does help the model to find the most up-to-date data without the need to be retrained, and this makes it great for circumstances where information changes often.  \n",
    "\n",
    "**Q3.** What are the advantages and disadvantages of RAG as opposed to supervised fine-tuning for improving model performance on a particular dataset (such as these wikipedia articles)?\n",
    "\n",
    "**A3.** One advantage of RAG that I can think of is that it can use outside information and can stay updated without the need to retrain the entire model. Also, it is flexible and can easily handle new data or information. A disadvantage of RAG is that it could occasionally fetch the wrong or incomplete information which can make the model's answers less accurate compared to a model that has been carefully fine-tuned on a particular dataset."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26039e5507bd461fba0a7da9f8bea465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a5f227c5684426891c7e5f3414d1b67",
       "IPY_MODEL_dd69aeb8ca6c4a19b4ab037d27f18746",
       "IPY_MODEL_ea1f8a63fa89419f9508d887dd6cf6d1"
      ],
      "layout": "IPY_MODEL_436bd4034fd0403c861cabaa5bbfb00d"
     }
    },
    "436bd4034fd0403c861cabaa5bbfb00d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a5f227c5684426891c7e5f3414d1b67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5610042d8cd4621b6de9e4874abbfd9",
      "placeholder": "​",
      "style": "IPY_MODEL_89e17778e2e04dd79d1094c6e2c0a786",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "758868075e884aef92014f81fdb0eff9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83659b0dcb7e4dfaa47fb2d78ef34c21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89e17778e2e04dd79d1094c6e2c0a786": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5610042d8cd4621b6de9e4874abbfd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da0af3e63b4b4f2bb0fdb21e24189bef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd69aeb8ca6c4a19b4ab037d27f18746": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da0af3e63b4b4f2bb0fdb21e24189bef",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_758868075e884aef92014f81fdb0eff9",
      "value": 2
     }
    },
    "ea1f8a63fa89419f9508d887dd6cf6d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83659b0dcb7e4dfaa47fb2d78ef34c21",
      "placeholder": "​",
      "style": "IPY_MODEL_eff498f319934ce28544c02f63f57af6",
      "value": " 2/2 [00:24&lt;00:00, 10.35s/it]"
     }
    },
    "eff498f319934ce28544c02f63f57af6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
