{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: MLP\n",
    "\n",
    "In this part of the assignment, you will complete a Do-It-Yourself (DIY) implementation of a multilayer perceptron (MLP), including code to optimize the network with backpropagation and minibatch stochastic gradient descent, that corresponds to the Scikit-Learn API.\n",
    "\n",
    "**Learning objectives.** You will:\n",
    "1. Write object-oriented code for a neural network class in Python, matching standard API patterns.\n",
    "2. Apply numerical Python (NumPy) to efficiently implement a multilayer perceptron with backpropagation and minibatch stochastic gradient descent. \n",
    "3. Evaluate your implementation compared to the Scikit-Learn standard on real image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code imports a dataset consisting of 8 by 8 pixel grayscale images of handwritten digits. Some images are visualized and then the data are flattened into 64-value one-dimensional NumPy arrays before splitting into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1797, 64)\n",
      "Number of classes: 10\n",
      "Training set size: 1257\n",
      "Test set size: 540\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run but DO NOT MODIFY this code\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "print(f\"Dataset shape: {digits.data.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(digits.target))}\")\n",
    "\n",
    "# visualizing examples\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)\n",
    "\n",
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Split data into 70% train and 30% test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.3, shuffle=False\n",
    "    )\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Using Scikit-Learn, build and train a [multilayer perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) fitted on the training data with one hidden layer to predict the value of `y` given `X`. \n",
    "\n",
    "**Configuration guidance:**\n",
    "- Use `hidden_layer_sizes` between (50,) and (200,). Start with (100,) as a baseline.\n",
    "- Set `max_iter=500` to allow sufficient training time\n",
    "- Use `random_state=2025` for reproducibility\n",
    "\n",
    "Ensure that you achieve a **training** [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of at least 90%.   \n",
    "\n",
    "Then evaluate and report the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of your model on the **test** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 1.0\n",
      "The test accuracy is 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Write code for task 1 here\n",
    "\n",
    "MLP_model = MLPClassifier(hidden_layer_sizes = (100,), max_iter = 500, random_state = 2025)\n",
    "MLP_model.fit(X_train, y_train)\n",
    "y_prediction_train = MLP_model.predict(X_train)\n",
    "y_prediction_test = MLP_model.predict(X_test)\n",
    "train_accuracy = accuracy_score(y_train, y_prediction_train)\n",
    "test_accuracy = accuracy_score(y_test, y_prediction_test)\n",
    "print(\"The training accuracy is\", train_accuracy)\n",
    "print(\"The test accuracy is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Background for DIY Implementation\n",
    "\n",
    "For your DIY implementation, you will build a neural network with the following architecture:\n",
    "- **Input layer**: 64 features (flattened 8Ã—8 images)\n",
    "- **Hidden layer**: Variable size with ReLU activation function\n",
    "- **Output layer**: 10 units (one per digit class) with softmax activation\n",
    "- **Loss function**: Cross-entropy loss\n",
    "\n",
    "The key equations for forward and backward propagation in the network are derived below for your convenience during implementation.\n",
    "\n",
    "### Key Equations\n",
    "**Notation**\n",
    "- `X`: Input design matrix\n",
    "- `W1`: (in_size, h_size) matrix of model parameters between input and hidden units\n",
    "- `z1`: pre-activation for hidden units\n",
    "- `a1`: post-activation for hidden units\n",
    "- `W2` (h_size, out_size) matrix of model parameters between hidden and output units\n",
    "- `z2` unnormalized output values\n",
    "- `y_pred` normalized output values: Probability distribution over classes\n",
    "- `y_onehot` One-hot encoding of true class membership label\n",
    "\n",
    "**Forward Pass:**\n",
    "1. Hidden layer: `z1 = X @ W1`, `a1 = relu(z1)` (computing the nonlinear activation of each unit)\n",
    "2. Output layer: `z2 = a1 @ W2`, `y_pred = softmax(z2, axis=1)` (apply softmax to each row)\n",
    "\n",
    "**Loss Function:**\n",
    "- Average cross-entropy: `L = -np.sum(y_onehot * np.log(y_pred + 1e-15)) / len(y_onehot)` where `y` is one-hot encoded and adding `1e-15` (or $10^{-15}$) is for numerical stability to prevent taking the logarithm of 0.\n",
    "\n",
    "**Backward Pass (Gradients):** The derivatives of the loss with respect to model parameters are:\n",
    "1. Output layer gradient: `dW2 = a1.T @ (y_pred - y_onehot)`\n",
    "2. Hidden layer gradient: `dW1 = X.T @ ((y_pred - y_onehot) @ W2.T * (z1 > 0))`\n",
    "\n",
    "(Note that `.T` is the transpose of a matrix and `(z1 > 0)` is the derivative of ReLU function )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Implement a do it yourself (DIY) multilayer perceptron classifier following the Scikit-Learn API by completing the `NeuralNet` class below. **You may not use `sklearn` for this task -- the point is for you to implement your own multilayer perceptron.**\n",
    "\n",
    "**Architecture specifications:**\n",
    "- Single hidden layer with ReLU activation\n",
    "- Output layer with softmax activation for multiclass classification\n",
    "- Cross-entropy loss function\n",
    "- Minibatch stochastic gradient descent optimization\n",
    "\n",
    "**Implementation tips and notes:**\n",
    "\n",
    "  - **Data format**: The Scikit-Learn API treats an input `X` array as a matrix with a row for every data point and a column for every feature.\n",
    "\n",
    "  - **One-hot encoding targets**: Convert integer labels (`y`) to one-hot vectors for cross-entropy loss. For example, the label `3` becomes `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]` - a vector with 1 at position 3 and 0s elsewhere. The `to_onehot` method is implemented for you.\n",
    "\n",
    "  - **Initialization**: Note that (slightly deviating from the `sklearn` API for specificity) the `init` method (the constructor) has two mandatory parameters to specify the input size (number of features) and output size (number of classes). This allows you to initialize the model weights as instance variables in the `init` method. This is already done for you.\n",
    "\n",
    "  - **Forward Passes and Predictions**: The three methods `forward`, `predict_proba`, and `predict` are all very related and should be implemented in this order, with each subsequent method calling the previous. Note that `forward` should return all intermediate calculations (these will be needed by `backward`). `predict_proba` returns a 2D array (row per input data point / row in `X`, column per class) whereas `predict` only returns a 1D array with a class prediction for each data point / row in `X`.\n",
    "\n",
    "  - **Backprop and Training**: The `backward` method computes derivatives of the loss with respect to model parameters `W1` and `W2` given input data `X` with predictive targets in one-hot encoding `y_onehot` and all of the information returned by the `forward` method. The `fit` method should optimize model parameters `W1` and `W2` using minibatch SGD as outlined. Update the model weights stored as instance variables. Keep track of evaluation information such as the average training loss or the training accuracy to print per epoch when `verbose=True`.\n",
    "   \n",
    "  - **Vectorization**: Use vectorized NumPy operations for efficiency. Avoid loops over large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for task 2 here\n",
    "\n",
    "# You can use the scipy implementation of softmax\n",
    "from scipy.special import softmax\n",
    "\n",
    "# You can use the numpy argmax and maximum functions\n",
    "from numpy import argmax, maximum\n",
    "\n",
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    MLP for multiclass classification with:\n",
    "    - Single hidden layer with ReLU activation\n",
    "    - Output layer with softmax activation\n",
    "    - Cross-entropy loss function\n",
    "    - Minibatch SGD optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, out_size, \n",
    "                 h_size=100, lr=0.001, batch_size=32,\n",
    "                 epochs=10, random_state=2025):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        - in_size: number of input features\n",
    "        - out_size: number of output classes\n",
    "        - h_size: number of hidden units\n",
    "        - lr: learning rate\n",
    "        - batch_size: minibatch size\n",
    "        - epochs: number of training epochs\n",
    "        - random_state: random seed\n",
    "        \"\"\"\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.h_size = h_size\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(random_state)\n",
    "        self.W1 = np.random.normal(0, 0.1, size=(in_size, h_size))\n",
    "        self.W2 = np.random.normal(0, 0.1, size=(h_size, out_size))\n",
    "    \n",
    "    def to_onehot(self, y):\n",
    "        \"\"\"Convert integer labels to one-hot encoding\"\"\"\n",
    "        return np.eye(self.out_size)[y]\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"Compute ReLU activation\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Softmax normalization applied per row\"\"\"\n",
    "        return softmax(z, axis=1)\n",
    "    \n",
    "    def shuffle_data(self, X, y_onehot):\n",
    "        \"\"\"Shuffle training data and labels together\"\"\"\n",
    "        indices = np.random.permutation(len(X))\n",
    "        return X[indices], y_onehot[indices]\n",
    "\n",
    "    def get_batch(self, X, y_onehot, start_idx, batch_size):\n",
    "        \"\"\"Get a minibatch of data starting at start_idx\"\"\"\n",
    "        end_idx = min(start_idx + batch_size, len(X))\n",
    "        return X[start_idx:end_idx], y_onehot[start_idx:end_idx]\n",
    "    \n",
    "    # YOUR TODOS BEGIN BELOW THIS POINT\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass returning intermediate values for backprop\"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Hint: Use the forward equations derived above\n",
    "        # Return z1, a1, z2, y_pred\n",
    "        z1 = X @ self.W1\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = a1 @ self.W2\n",
    "        y_pred = self.softmax(z2)\n",
    "        return z1, a1, z2, y_pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities for each row in X for each class\"\"\"\n",
    "        # TODO: Implement predict_proba\n",
    "        # Hint: Use the forward method\n",
    "        z1, a1, z2, y_pred = self.forward(X)\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for each row in X\"\"\"\n",
    "        # TODO: Use predict_proba and return class with highest probability\n",
    "        # Hint: Use numpy's argmax function\n",
    "        probabilities = self.predict_proba(X)\n",
    "        results = np.argmax(probabilities, axis = 1)\n",
    "        return results\n",
    "     \n",
    "    def backward(self, X, y_onehot, z1, a1, z2, y_pred):\n",
    "        \"\"\"Compute gradients using backpropagation\"\"\"\n",
    "        # TODO: Implement backward pass\n",
    "        # Hint: Use the backprop equations derived above\n",
    "        # Return dW1, dW2\n",
    "        dW2 = a1.T @ (y_pred - y_onehot)\n",
    "        dW1 = X.T @ ((y_pred - y_onehot) @ self.W2.T * (z1 > 0))\n",
    "        return dW1, dW2\n",
    "     \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"Train on inputs X with labels y using backprop and minibatch SGD\"\"\"\n",
    "        # TODO: Complete fit implementation\n",
    "        # Convert y to one-hot encoding\n",
    "        # Training loop over epochs\n",
    "        #   - Shuffle data\n",
    "        #   - Loop over minibatches\n",
    "        #     - forward pass\n",
    "        #     - backward pass\n",
    "        #     - update weights\n",
    "        #   - Print progress (training loss and/or accuracy) if verbose\n",
    "        y_onehot_encoding = self.to_onehot(y)\n",
    "        for epoch in range(self.epochs):\n",
    "            X_shuffle, y_shuffle = self.shuffle_data(X, y_onehot_encoding)\n",
    "            for start_idx in range(0, len(X_shuffle), self.batch_size):\n",
    "                X_batch, y_batch = self.get_batch(X_shuffle, y_shuffle, start_idx, self.batch_size)\n",
    "                z1, a1, z2, y_pred = self.forward(X_batch)\n",
    "                dW1, dW2 = self.backward(X_batch, y_batch, z1, a1, z2, y_pred)\n",
    "                self.W1 -= self.lr * dW1\n",
    "                self.W2 -= self.lr * dW2\n",
    "            if verbose:\n",
    "                full_z1, full_a1, full_z2, full_y_pred = self.forward(X)\n",
    "                L = -np.sum(y_onehot_encoding * np.log(full_y_pred + 1e-15)) / len(y_onehot_encoding)\n",
    "                accuracy = (np.argmax(full_y_pred, axis = 1) == y).mean()\n",
    "                print(\"Epoch\", epoch + 1, \": Loss =\", L, \", Accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Show that your `NeuralNet` class can effectively learn. Fit an instance of your DIY model on the training data to predict the value of `y` given `X`. \n",
    "\n",
    "**Hyperparameter guidance:** While you can choose your own hyperparameters and experiment as you see fit, we recommend the following as reasonable starting points:\n",
    "- Start with `h_size` around `100` to `500` for the hidden layer\n",
    "- Use a learning rate `lr` between `0.005` and `0.01` for your learning rate\n",
    "- Use a `batch_size` of 16 to 64 for stable training\n",
    "- Begin with `epochs=100` and adjust based on convergence\n",
    "- Set `verbose=True` to monitor training progress (you should see loss decreasing over epochs and accuracy increasing)\n",
    "\n",
    "Ensure that you achieve a **training** [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of at least 80% (your implementation may not match scikit-learn's optimized performance, but should demonstrate competitive functionality).   \n",
    "\n",
    "Then evaluate and report the [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of your model on the **test** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 0.8763122991859336 , Accuracy = 0.7064439140811456\n",
      "Epoch 2 : Loss = 0.3034165046301553 , Accuracy = 0.888623707239459\n",
      "Epoch 3 : Loss = 0.3853998914720612 , Accuracy = 0.8846459824980112\n",
      "Epoch 4 : Loss = 0.21104494618364134 , Accuracy = 0.9443118536197295\n",
      "Epoch 5 : Loss = 0.14184336840717657 , Accuracy = 0.9681782020684169\n",
      "Epoch 6 : Loss = 0.08718757306743806 , Accuracy = 0.9824980111376292\n",
      "Epoch 7 : Loss = 0.03352708876662646 , Accuracy = 0.9888623707239459\n",
      "Epoch 8 : Loss = 0.15222353690720897 , Accuracy = 0.9554494828957836\n",
      "Epoch 9 : Loss = 0.06826868234108611 , Accuracy = 0.9745425616547335\n",
      "Epoch 10 : Loss = 0.23096585101052966 , Accuracy = 0.9355608591885441\n",
      "Epoch 11 : Loss = 0.017640517167690252 , Accuracy = 0.9936356404136834\n",
      "Epoch 12 : Loss = 0.024294345976962697 , Accuracy = 0.9920445505171042\n",
      "Epoch 13 : Loss = 0.03789438001355667 , Accuracy = 0.9856801909307876\n",
      "Epoch 14 : Loss = 0.015473551450653381 , Accuracy = 0.9952267303102625\n",
      "Epoch 15 : Loss = 0.008571118149038931 , Accuracy = 0.9984089101034208\n",
      "Epoch 16 : Loss = 0.005118586578493504 , Accuracy = 0.9984089101034208\n",
      "Epoch 17 : Loss = 0.0147229704501428 , Accuracy = 0.9952267303102625\n",
      "Epoch 18 : Loss = 0.014925739935257 , Accuracy = 0.9952267303102625\n",
      "Epoch 19 : Loss = 0.016466949469733348 , Accuracy = 0.994431185361973\n",
      "Epoch 20 : Loss = 0.0013635663556165933 , Accuracy = 1.0\n",
      "Epoch 21 : Loss = 0.0009533233865540653 , Accuracy = 1.0\n",
      "Epoch 22 : Loss = 0.0009892528126879349 , Accuracy = 1.0\n",
      "Epoch 23 : Loss = 0.000796474695865788 , Accuracy = 1.0\n",
      "Epoch 24 : Loss = 0.0006492876893642553 , Accuracy = 1.0\n",
      "Epoch 25 : Loss = 0.0005320693788495052 , Accuracy = 1.0\n",
      "Epoch 26 : Loss = 0.0004878454229130306 , Accuracy = 1.0\n",
      "Epoch 27 : Loss = 0.0004559730384579996 , Accuracy = 1.0\n",
      "Epoch 28 : Loss = 0.000438142693392767 , Accuracy = 1.0\n",
      "Epoch 29 : Loss = 0.0004121969480302706 , Accuracy = 1.0\n",
      "Epoch 30 : Loss = 0.0003936583624343459 , Accuracy = 1.0\n",
      "Epoch 31 : Loss = 0.00036444725838400836 , Accuracy = 1.0\n",
      "Epoch 32 : Loss = 0.0003494401935567289 , Accuracy = 1.0\n",
      "Epoch 33 : Loss = 0.0003365802128933526 , Accuracy = 1.0\n",
      "Epoch 34 : Loss = 0.00033083719732911356 , Accuracy = 1.0\n",
      "Epoch 35 : Loss = 0.0003253192041380705 , Accuracy = 1.0\n",
      "Epoch 36 : Loss = 0.0002997620917702583 , Accuracy = 1.0\n",
      "Epoch 37 : Loss = 0.0002906910299955277 , Accuracy = 1.0\n",
      "Epoch 38 : Loss = 0.0002794677749020081 , Accuracy = 1.0\n",
      "Epoch 39 : Loss = 0.0002723173516009556 , Accuracy = 1.0\n",
      "Epoch 40 : Loss = 0.00026581456242430153 , Accuracy = 1.0\n",
      "Epoch 41 : Loss = 0.0002552032747159275 , Accuracy = 1.0\n",
      "Epoch 42 : Loss = 0.000250867527672801 , Accuracy = 1.0\n",
      "Epoch 43 : Loss = 0.00024035963100540734 , Accuracy = 1.0\n",
      "Epoch 44 : Loss = 0.00023522872191457928 , Accuracy = 1.0\n",
      "Epoch 45 : Loss = 0.0002299876257208161 , Accuracy = 1.0\n",
      "Epoch 46 : Loss = 0.00022467654035696175 , Accuracy = 1.0\n",
      "Epoch 47 : Loss = 0.0002184023723746313 , Accuracy = 1.0\n",
      "Epoch 48 : Loss = 0.00021165240994574255 , Accuracy = 1.0\n",
      "Epoch 49 : Loss = 0.0002070449775905299 , Accuracy = 1.0\n",
      "Epoch 50 : Loss = 0.0002027853646882076 , Accuracy = 1.0\n",
      "Epoch 51 : Loss = 0.0001974334181201264 , Accuracy = 1.0\n",
      "Epoch 52 : Loss = 0.00019432729301116427 , Accuracy = 1.0\n",
      "Epoch 53 : Loss = 0.00019085461947941247 , Accuracy = 1.0\n",
      "Epoch 54 : Loss = 0.00018539561237665864 , Accuracy = 1.0\n",
      "Epoch 55 : Loss = 0.00018171035563657747 , Accuracy = 1.0\n",
      "Epoch 56 : Loss = 0.00017787659882103225 , Accuracy = 1.0\n",
      "Epoch 57 : Loss = 0.0001750356100553148 , Accuracy = 1.0\n",
      "Epoch 58 : Loss = 0.00017095058569446657 , Accuracy = 1.0\n",
      "Epoch 59 : Loss = 0.00016835372020463816 , Accuracy = 1.0\n",
      "Epoch 60 : Loss = 0.0001643556067441824 , Accuracy = 1.0\n",
      "Epoch 61 : Loss = 0.00016128367060365317 , Accuracy = 1.0\n",
      "Epoch 62 : Loss = 0.00015856044498994984 , Accuracy = 1.0\n",
      "Epoch 63 : Loss = 0.00015644771761932425 , Accuracy = 1.0\n",
      "Epoch 64 : Loss = 0.00015360100293394156 , Accuracy = 1.0\n",
      "Epoch 65 : Loss = 0.00015063871634162299 , Accuracy = 1.0\n",
      "Epoch 66 : Loss = 0.0001478184495101477 , Accuracy = 1.0\n",
      "Epoch 67 : Loss = 0.0001460057380263107 , Accuracy = 1.0\n",
      "Epoch 68 : Loss = 0.00014353056783250137 , Accuracy = 1.0\n",
      "Epoch 69 : Loss = 0.00014114298594133576 , Accuracy = 1.0\n",
      "Epoch 70 : Loss = 0.00013889071160544378 , Accuracy = 1.0\n",
      "Epoch 71 : Loss = 0.00013700903742649216 , Accuracy = 1.0\n",
      "Epoch 72 : Loss = 0.0001347746597909831 , Accuracy = 1.0\n",
      "Epoch 73 : Loss = 0.00013289333914936417 , Accuracy = 1.0\n",
      "Epoch 74 : Loss = 0.00013110916017555185 , Accuracy = 1.0\n",
      "Epoch 75 : Loss = 0.00012914726071809474 , Accuracy = 1.0\n",
      "Epoch 76 : Loss = 0.0001274469651860172 , Accuracy = 1.0\n",
      "Epoch 77 : Loss = 0.0001258283063884257 , Accuracy = 1.0\n",
      "Epoch 78 : Loss = 0.0001242314579341322 , Accuracy = 1.0\n",
      "Epoch 79 : Loss = 0.00012235776790067072 , Accuracy = 1.0\n",
      "Epoch 80 : Loss = 0.0001207985085950353 , Accuracy = 1.0\n",
      "Epoch 81 : Loss = 0.00011935947163922948 , Accuracy = 1.0\n",
      "Epoch 82 : Loss = 0.00011783606056438917 , Accuracy = 1.0\n",
      "Epoch 83 : Loss = 0.00011634627954272901 , Accuracy = 1.0\n",
      "Epoch 84 : Loss = 0.00011486055570186824 , Accuracy = 1.0\n",
      "Epoch 85 : Loss = 0.00011350160077948012 , Accuracy = 1.0\n",
      "Epoch 86 : Loss = 0.00011228535707880831 , Accuracy = 1.0\n",
      "Epoch 87 : Loss = 0.00011088955675923695 , Accuracy = 1.0\n",
      "Epoch 88 : Loss = 0.00010968023758558031 , Accuracy = 1.0\n",
      "Epoch 89 : Loss = 0.00010838556344337255 , Accuracy = 1.0\n",
      "Epoch 90 : Loss = 0.00010714533305300288 , Accuracy = 1.0\n",
      "Epoch 91 : Loss = 0.000106080159984176 , Accuracy = 1.0\n",
      "Epoch 92 : Loss = 0.00010482864818572709 , Accuracy = 1.0\n",
      "Epoch 93 : Loss = 0.00010370301014137918 , Accuracy = 1.0\n",
      "Epoch 94 : Loss = 0.00010274135705760734 , Accuracy = 1.0\n",
      "Epoch 95 : Loss = 0.00010160420429447116 , Accuracy = 1.0\n",
      "Epoch 96 : Loss = 0.00010046095780696616 , Accuracy = 1.0\n",
      "Epoch 97 : Loss = 9.952350871094503e-05 , Accuracy = 1.0\n",
      "Epoch 98 : Loss = 9.846644184080235e-05 , Accuracy = 1.0\n",
      "Epoch 99 : Loss = 9.748840131014698e-05 , Accuracy = 1.0\n",
      "Epoch 100 : Loss = 9.651527270195742e-05 , Accuracy = 1.0\n",
      "The training accuracy is 1.0\n",
      "The test accuracy is 0.9351851851851852\n",
      "The training accuracy from Task 1 is 1.0\n",
      "The test accuracy from Task 1 is 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Write code for task 3 here\n",
    "\n",
    "neuralnet_model = NeuralNet(in_size = 64, out_size = 10, h_size = 100, lr = 0.005, batch_size = 16, epochs = 100)\n",
    "neuralnet_model.fit(X_train, y_train, verbose = True)\n",
    "y_prediction_train_neuralnet = neuralnet_model.predict(X_train)\n",
    "y_prediction_test_neuralnet = neuralnet_model.predict(X_test)\n",
    "neuralnet_train_accuracy = accuracy_score(y_train, y_prediction_train_neuralnet)\n",
    "neural_test_accuracy = accuracy_score(y_test, y_prediction_test_neuralnet)\n",
    "print(\"The training accuracy is\", neuralnet_train_accuracy)\n",
    "print(\"The test accuracy is\", neural_test_accuracy)\n",
    "print(\"The training accuracy from Task 1 is\", train_accuracy)\n",
    "print(\"The test accuracy from Task 1 is\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Hyperparameter Ablation Study\n",
    "\n",
    "Understanding how hyperparameters affect model performance is crucial for training effective neural networks. In this task, you will systematically test different learning rates and batch sizes to observe their impact on training convergence.\n",
    "\n",
    "**What you'll investigate:** You will test whether different hyperparameter combinations can achieve good performance (>80% training accuracy) and how quickly they converge to this threshold.\n",
    "\n",
    "1. Test different learning rates using your `NeuralNet` class. Leave all other hyperparameters at their default values, varying only the learning rate `lr`.\n",
    "    - Learning rates to test: `[0.0001, 0.001, 0.01]`\n",
    "    - Set `verbose=True` to monitor training progress\n",
    "\n",
    "2. Test different batch sizes using your `NeuralNet` class. Leave all other hyperparameters at their default values, varying only the batch size `batch_size`.\n",
    "    - Batch sizes to test: `[16, 128, 1024]`\n",
    "    - Set `verbose=True` to monitor training progress\n",
    "\n",
    "3. Record your results in the table format below. For each hyperparameter setting, note how many epochs were needed to reach >80% training accuracy. For example, if the network achieves >80% accuracy after the first epoch, write 1 in table. If after second epoch the accuracy >80%, write 2. If a given setting failed to reach that accuracy in the default maximum number of epochs, you can just write N/A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 0.0001 as the learning rate and default batch size.\n",
      "Epoch 1 : Loss = 0.9756647362501694 , Accuracy = 0.6945107398568019\n",
      "Epoch 2 : Loss = 0.5506590100288823 , Accuracy = 0.8424821002386634\n",
      "Epoch 3 : Loss = 0.38689279027840706 , Accuracy = 0.8846459824980112\n",
      "Epoch 4 : Loss = 0.3045097256437147 , Accuracy = 0.9220365950676214\n",
      "Epoch 5 : Loss = 0.2558975287647104 , Accuracy = 0.939538583929992\n",
      "Epoch 6 : Loss = 0.21771988687374022 , Accuracy = 0.9498806682577565\n",
      "Epoch 7 : Loss = 0.1908185162085133 , Accuracy = 0.9562450278440732\n",
      "Epoch 8 : Loss = 0.17560785157225672 , Accuracy = 0.9594272076372315\n",
      "Epoch 9 : Loss = 0.1570988682154445 , Accuracy = 0.9665871121718377\n",
      "Epoch 10 : Loss = 0.14492625461482714 , Accuracy = 0.964200477326969\n",
      "\n",
      "Model with 0.001 as the learning rate and default batch size.\n",
      "Epoch 1 : Loss = 0.30012486018799567 , Accuracy = 0.9053301511535402\n",
      "Epoch 2 : Loss = 0.12621570080263425 , Accuracy = 0.9737470167064439\n",
      "Epoch 3 : Loss = 0.1045047223862448 , Accuracy = 0.9657915672235481\n",
      "Epoch 4 : Loss = 0.06422375028346523 , Accuracy = 0.9896579156722355\n",
      "Epoch 5 : Loss = 0.056671319433682955 , Accuracy = 0.9872712808273667\n",
      "Epoch 6 : Loss = 0.044938597713877815 , Accuracy = 0.9928400954653938\n",
      "Epoch 7 : Loss = 0.030770247482371903 , Accuracy = 0.9952267303102625\n",
      "Epoch 8 : Loss = 0.06319243905877987 , Accuracy = 0.9824980111376292\n",
      "Epoch 9 : Loss = 0.02449354871010163 , Accuracy = 0.9976133651551312\n",
      "Epoch 10 : Loss = 0.03927463111655006 , Accuracy = 0.9896579156722355\n",
      "\n",
      "Model with 0.01 as the learning rate and default batch size.\n",
      "Epoch 1 : Loss = 2.2833192995990195 , Accuracy = 0.09785202863961814\n",
      "Epoch 2 : Loss = 2.300696858587816 , Accuracy = 0.07637231503579953\n",
      "Epoch 3 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "Epoch 4 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "Epoch 5 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "Epoch 6 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "Epoch 7 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "Epoch 8 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "Epoch 9 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "Epoch 10 : Loss = 2.3025850929940357 , Accuracy = 0.09944311853619729\n",
      "\n",
      "Model with default learning rate and 16 as the batch size.\n",
      "Epoch 1 : Loss = 0.30325398133383336 , Accuracy = 0.8973747016706444\n",
      "Epoch 2 : Loss = 0.10371567726433437 , Accuracy = 0.9737470167064439\n",
      "Epoch 3 : Loss = 0.06899867382783888 , Accuracy = 0.9809069212410502\n",
      "Epoch 4 : Loss = 0.048171017312148706 , Accuracy = 0.9920445505171042\n",
      "Epoch 5 : Loss = 0.04087443536185222 , Accuracy = 0.994431185361973\n",
      "Epoch 6 : Loss = 0.03871443265878085 , Accuracy = 0.9928400954653938\n",
      "Epoch 7 : Loss = 0.025135985607579913 , Accuracy = 0.9984089101034208\n",
      "Epoch 8 : Loss = 0.03187946600132063 , Accuracy = 0.9936356404136834\n",
      "Epoch 9 : Loss = 0.020004302179151175 , Accuracy = 0.9984089101034208\n",
      "Epoch 10 : Loss = 0.04397215366099512 , Accuracy = 0.9880668257756563\n",
      "\n",
      "Model with default learning rate and 128 as the batch size.\n",
      "Epoch 1 : Loss = 1.2830485224490682 , Accuracy = 0.49801113762927607\n",
      "Epoch 2 : Loss = 1.398279659353418 , Accuracy = 0.49801113762927607\n",
      "Epoch 3 : Loss = 0.5425609872102928 , Accuracy = 0.8369132856006364\n",
      "Epoch 4 : Loss = 0.6773980576504433 , Accuracy = 0.7836117740652346\n",
      "Epoch 5 : Loss = 0.20116057699484957 , Accuracy = 0.9379474940334129\n",
      "Epoch 6 : Loss = 0.1240901423624332 , Accuracy = 0.964200477326969\n",
      "Epoch 7 : Loss = 1.0014812081067315 , Accuracy = 0.711217183770883\n",
      "Epoch 8 : Loss = 0.25119325020581573 , Accuracy = 0.9252187748607796\n",
      "Epoch 9 : Loss = 0.18767511259958305 , Accuracy = 0.9363564041368337\n",
      "Epoch 10 : Loss = 0.1237429902444627 , Accuracy = 0.9634049323786794\n",
      "\n",
      "Model with default learning rate and 1024 as the batch size.\n",
      "Epoch 1 : Loss = 30.2799095744911 , Accuracy = 0.10103420843277645\n",
      "Epoch 2 : Loss = 30.896054827279073 , Accuracy = 0.10421638822593476\n",
      "Epoch 3 : Loss = 8.352884767406032 , Accuracy = 0.10103420843277645\n",
      "Epoch 4 : Loss = 2.2993422676765554 , Accuracy = 0.1105807478122514\n",
      "Epoch 5 : Loss = 2.2058446683789654 , Accuracy = 0.147175815433572\n",
      "Epoch 6 : Loss = 6.918524351616987 , Accuracy = 0.10103420843277645\n",
      "Epoch 7 : Loss = 2.3025230363727975 , Accuracy = 0.09785202863961814\n",
      "Epoch 8 : Loss = 2.3024782310532554 , Accuracy = 0.09785202863961814\n",
      "Epoch 9 : Loss = 2.3024336586925447 , Accuracy = 0.09785202863961814\n",
      "Epoch 10 : Loss = 2.3023861005172352 , Accuracy = 0.09626093874303898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write code for experimenting in task 4 here\n",
    "# Your ablation study does not have to be automated,\n",
    "# it is fine to run this cell multiple times and just\n",
    "# record your results below.\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [16, 128, 1024]\n",
    "for lr in learning_rates:\n",
    "    print(\"Model with\", lr, \"as the learning rate and default batch size.\")\n",
    "    model = NeuralNet(in_size = 64, out_size = 10, lr = lr)\n",
    "    model.fit(X_train, y_train, verbose = True)\n",
    "    print()\n",
    "for bs in batch_sizes:\n",
    "    print(\"Model with default learning rate and\", bs, \"as the batch size.\")\n",
    "    model = NeuralNet(in_size = 64, out_size = 10, batch_size = bs)\n",
    "    model.fit(X_train, y_train, verbose = True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill in your experimental results for task 4 here\n",
    "\n",
    "| Learning Rate | Batch Size  | Epochs to reach >80% accuracy  |\n",
    "|---------------|-------------|--------------------------------|\n",
    "| 0.0001        | [default]   |         2        |\n",
    "| 0.001         | [default]   |         1        |\n",
    "| 0.01          | [default]   |         N/A        |\n",
    "| [default]     | 16          |         1        |\n",
    "| [default]     | 128         |         3        |\n",
    "| [default]     | 1024        |         N/A        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
